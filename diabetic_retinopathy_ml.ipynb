{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diabetic-retinopathy-ml.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G7vnddzcvjMR"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN5znD6g_vFV",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokarchyn/diabetic-retinopathy-thesis/blob/master/diabetic-retinopathy-ml.ipynb\" target=\"_parent\">     \n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-XAUTpUtth",
        "colab_type": "text"
      },
      "source": [
        "# Imports and methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQKx-pJN_BNx",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import argparse\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "from IPython.display import display, clear_output\n",
        "import glob\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# %precision % .5f\n",
        "np.set_printoptions(suppress=True, precision=5)\n",
        "\n",
        "# Define constants\n",
        "CLASS_NAMES = np.array(\n",
        "    ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR'])\n",
        "CLASS_INDEXES = [0, 1, 2, 3, 4]\n",
        "\n",
        "\n",
        "def fetch_data_from_gdrive(project_dir, remote_project_dir, zip_name):\n",
        "    target_zip = os.path.join(project_dir, zip_name)\n",
        "    !mkdir -p \"{project_dir}\"\n",
        "    if not os.path.exists(target_zip):\n",
        "        !cp \"{remote_project_dir}/{zip_name}\" \"{target_zip}\"\n",
        "    if not os.path.exists(Path(target_zip).stem):\n",
        "        !unzip -q \"{target_zip}\"\n",
        "        !rm \"{target_zip}\"\n",
        "\n",
        "\n",
        "def init_env():\n",
        "    args = {}\n",
        "    if IN_COLAB:\n",
        "        args['remote_project_dir'] = 'drive/My Drive/diabetic-retinopathy-thesis'\n",
        "        args['project_dir'] = '/content'\n",
        "        args['image_dir'] = os.path.join(\n",
        "            args['project_dir'], 'train_processed')\n",
        "        args['dataframe_path'] = os.path.join(\n",
        "            args['remote_project_dir'], 'trainLabels.csv')\n",
        "        args['experiments_dir'] = os.path.join(\n",
        "            args['remote_project_dir'], 'experiments')\n",
        "        args['quality_dataset_path'] = None\n",
        "        args['gpu_id'] = None\n",
        "\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        fetch_data_from_gdrive(args['project_dir'],\n",
        "                               args['remote_project_dir'],\n",
        "                               'train_processed.zip')\n",
        "    else:\n",
        "        old_argv = sys.argv\n",
        "        if sys.argv[-1].endswith('json'):\n",
        "            sys.argv = ['']\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--image_dir', type=str, default='train_processed')\n",
        "        parser.add_argument('--dataframe_path', type=str,\n",
        "                            default='trainLabels_full.csv')\n",
        "        parser.add_argument('--quality_dataset_path',\n",
        "                            type=str, default=None)\n",
        "        parser.add_argument('--experiments_dir', type=str,\n",
        "                            default='experiments')\n",
        "        parser.add_argument('--gpu_id', type=str,\n",
        "                            default=None)\n",
        "        parsed_args = parser.parse_args()\n",
        "        args = vars(parsed_args)\n",
        "        sys.argv = old_argv\n",
        "\n",
        "    if args['gpu_id'] is not None:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"]=args['gpu_id']\n",
        "        tf_device='/gpu:0'\n",
        "\n",
        "    args['img_size'] = 512\n",
        "    args['batch_size'] = 16\n",
        "    print('Arguments:', json.dumps(args))\n",
        "    return args\n",
        "\n",
        "# Methods to process dataframe\n",
        "\n",
        "\n",
        "def load_df(dataframe_path, base_image_dir):\n",
        "    df = pd.read_csv(dataframe_path)\n",
        "    df['image_path'] = df['image'].astype(str).apply(\n",
        "        lambda x: os.path.join(base_image_dir, x + '.jpeg'))\n",
        "    df = df.drop(columns=['image'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_unexist(df, base_image_dir):\n",
        "    all_images = glob.glob(base_image_dir + \"/*\")\n",
        "    while len(all_images) == 0:\n",
        "        all_images = glob.glob(base_image_dir + \"/*\")\n",
        "    print('Found', len(all_images), 'images')\n",
        "    df['exists'] = df['image_path'].map(lambda p: p in all_images)\n",
        "    df = df[df['exists']].drop(columns=['exists'])\n",
        "    print('Number of existed images is', len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "def train_val_split(df):\n",
        "    train_img, val_img = train_test_split(df['image_path'],\n",
        "                                          test_size=0.20,\n",
        "                                          random_state=2020,\n",
        "                                          stratify=df['level'].to_frame())\n",
        "    train_df = df[df['image_path'].isin(train_img)]\n",
        "    val_df = df[df['image_path'].isin(val_img)]\n",
        "    print('Train dataframe size:',\n",
        "          train_df.shape[0], 'Validation dataframe size:', val_df.shape[0])\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def calc_weights(df):\n",
        "    level_counts = df['level'].value_counts().sort_index()\n",
        "    weights = {cls: len(df) / count for cls, count in enumerate(level_counts)}\n",
        "    if max(weights.values()) - min(weights.values()) < 0.1:\n",
        "        print('Reset weights because they all are the same:', max(weights.values()))\n",
        "        weights = None\n",
        "    else:\n",
        "        print('Weights for each level:\\n', weights)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def balance(df, counts):\n",
        "    new_df = df.iloc[0:0]  # copy only structure\n",
        "    for level in df['level'].unique():\n",
        "        df_level = df[df['level'] == level]\n",
        "        count = len(df_level)\n",
        "        new_count = counts[level] if level in counts else count\n",
        "        if count > new_count:\n",
        "            new_df = new_df.append(\n",
        "                df_level.drop(df_level.sample(count - new_count).index),\n",
        "                ignore_index=True)\n",
        "        elif count < new_count:\n",
        "            new_df = new_df.append(df_level, ignore_index=True)\n",
        "            new_df = new_df.append(df_level.sample(\n",
        "                new_count - count, replace=True), ignore_index=True)\n",
        "\n",
        "    print('New counts of dataset\\'s categories: ', json.dumps(\n",
        "        new_df['level'].value_counts().to_dict()))\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def balance_with_mode(df, mode='max'):\n",
        "    counts = df['level'].value_counts()\n",
        "    new_count = 0\n",
        "    if mode == 'max':\n",
        "        new_count = counts.max()\n",
        "    elif mode == 'min':\n",
        "        new_count = counts.min()\n",
        "    new_counts_dict = {level: new_count for level in df['level'].unique()}\n",
        "    return balance(df, counts=new_counts_dict)\n",
        "\n",
        "\n",
        "def shuffle(df):\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def exclude_by_quality(df, quality_dataset_path):\n",
        "    quality_dict = pd.read_csv(quality_dataset_path, index_col='image_name')[\n",
        "        'quality'].to_dict()\n",
        "    select = df['image_path'].apply(lambda p: Path(\n",
        "        p).stem not in quality_dict or quality_dict[Path(p).stem] == 0)\n",
        "    print('Images to exclude:', len(select[select == False]))\n",
        "    df = df.loc[select]\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_data(dataframe_path, base_image_dir, quality_dataset_path=None):\n",
        "    if not os.path.exists(base_image_dir):\n",
        "        raise NameError('Base image path doesnt exist', base_image_dir)\n",
        "    df = load_df(dataframe_path, base_image_dir)\n",
        "    df = remove_unexist(df, base_image_dir)\n",
        "    df = shuffle(df)\n",
        "    # df = shrink_dataset(df, 1000)\n",
        "\n",
        "    train_df, val_df = train_val_split(df)\n",
        "    if quality_dataset_path is not None:\n",
        "        train_df = exclude_by_quality(train_df, quality_dataset_path)\n",
        "    # train_df = balance_with_mode(train_df, mode='max') # take the same number of samples as majority category has\n",
        "    # take some samples from each category\n",
        "    # train_df = balance(train_df, counts={0: 6000, 1: 6000, 2: 6000, 3: 6000, 4: 6000})\n",
        "    # train_df = balance_with_mode(train_df, mode='min') # take the same number of samples as minority category has\n",
        "    train_df = shuffle(train_df)\n",
        "    weights = calc_weights(train_df)\n",
        "\n",
        "    return train_df, val_df, weights\n",
        "\n",
        "# Augmentation functions\n",
        "\n",
        "\n",
        "def rotate(x):\n",
        "    # Rotate 0, 90, 180, 270 degrees\n",
        "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "\n",
        "\n",
        "def flip(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_flip_up_down(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def color(x):\n",
        "    x = tf.image.random_hue(x, 0.04)\n",
        "    x = tf.image.random_saturation(x, 0.9, 1.1)\n",
        "    x = tf.image.random_brightness(x, 0.04)\n",
        "    x = tf.image.random_contrast(x, 0.9, 1.1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def zoom(x, img_size):\n",
        "    # Generate 20 crop settings, ranging from a 1% to 10% crop.\n",
        "    scales = list(np.arange(0.9, 1, 0.01))\n",
        "    boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - (0.5 * scale)\n",
        "        x2 = y2 = 0.5 + (0.5 * scale)\n",
        "        boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "    def random_crop(img):\n",
        "        # Create different crops for an image\n",
        "        crops = tf.image.crop_and_resize([img], boxes=boxes,\n",
        "                                         box_indices=np.zeros(len(scales)),\n",
        "                                         crop_size=(img_size, img_size))\n",
        "        # Return a random crop\n",
        "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "\n",
        "    choice = tf.random.uniform(\n",
        "        shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "\n",
        "    # Only apply cropping 50% of the time\n",
        "    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n",
        "\n",
        "\n",
        "def augment(dataset, img_size, aug_probability=1):\n",
        "    def zoom_local(x): return zoom(x, img_size)\n",
        "    augmentations = [flip, rotate, color]\n",
        "\n",
        "    def augment_map(img, level, aug_fun):\n",
        "        return (aug_fun(img), level)\n",
        "        # return (tf.cond(tf.math.argmax(level, axis = 0) == 0, lambda: img, lambda: aug_fun(img)), level)\n",
        "        # choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "        # return (tf.cond(choice < aug_probability, lambda: img, lambda: aug_fun(img)),\n",
        "        #         level)\n",
        "\n",
        "    # Add the augmentations to the dataset\n",
        "    for f in augmentations:\n",
        "        dataset = dataset.map(lambda img, level: augment_map(\n",
        "            img, level, f), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Make sure that the values are still in [0, 1]\n",
        "    dataset = dataset.map(lambda img, level: (\n",
        "        tf.clip_by_value(img, 0, 1), level), num_parallel_calls=AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create Tensorflow's dataset\n",
        "\n",
        "\n",
        "def get_input_shape(img_size):\n",
        "    return (img_size, img_size, 3)\n",
        "\n",
        "\n",
        "def decode_img(img, img_size):\n",
        "    # convert the compressed string to a 3D uint8 tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resize the image to the desired size.\n",
        "    return tf.image.resize(img, [img_size, img_size])\n",
        "\n",
        "\n",
        "def get_label(level):\n",
        "    return tf.cast(level == CLASS_INDEXES, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def process_path(file_path, level, img_size):\n",
        "    label = get_label(level)\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img, img_size)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle_buffer_size=200):\n",
        "    ds = ds.map(lambda img, level: (tf.image.per_image_standardization(img), level),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "def dataset_from_tensor_slices(df):\n",
        "    return tf.data.Dataset.from_tensor_slices((\n",
        "        df['image_path'].to_numpy(copy=True),\n",
        "        df['level'].to_numpy(copy=True)))\n",
        "\n",
        "\n",
        "def create_datasets(train_df, val_df, img_size, batch_size):\n",
        "    train_ds = dataset_from_tensor_slices(train_df)\n",
        "    val_ds = dataset_from_tensor_slices(val_df)\n",
        "\n",
        "    def process_path_local(file_path, level): return process_path(\n",
        "        file_path, level, img_size)\n",
        "\n",
        "    train_ds = train_ds.map(process_path_local, num_parallel_calls=AUTOTUNE)\n",
        "    train_ds = augment(train_ds, img_size)\n",
        "    train_ds = prepare(train_ds)\n",
        "    train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "    val_ds = val_ds.map(process_path_local, num_parallel_calls=AUTOTUNE)\n",
        "    val_ds = prepare(val_ds)\n",
        "    val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "    return train_ds, len(train_df), val_ds, len(val_df)\n",
        "\n",
        "# Visualisation\n",
        "\n",
        "\n",
        "def show_batch(image_batch, label_batch, number_to_show=4, predicted_labels=None):\n",
        "    row_count = math.ceil(number_to_show / 4)\n",
        "    fig, m_axs = plt.subplots(row_count, 4, figsize=(16, row_count * 4))\n",
        "    for i, (c_x, c_y, c_ax) in enumerate(zip(image_batch, label_batch, m_axs.flatten())):\n",
        "        c_ax.imshow(c_x)\n",
        "        real_level = CLASS_NAMES[c_y == 1][0]\n",
        "        pred_level = ''\n",
        "        title = 'Real level: ' + real_level\n",
        "        if predicted_labels is not None:\n",
        "            pred_level = CLASS_NAMES[predicted_labels[i]]\n",
        "            title = title + '\\nPredicted one: ' + pred_level\n",
        "        c_ax.set_title(title, color='g' if pred_level ==\n",
        "                       '' or real_level == pred_level else 'r')\n",
        "        c_ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_metric(metrics, metric_name, save_dest=None):\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plt.plot(metrics[metric_name])\n",
        "    plt.plot(metrics['val_' + metric_name])\n",
        "    plt.title(metric_name)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "    if save_dest:\n",
        "        plt.savefig(os.path.join(save_dest, metric_name + '.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_f1(metrics, save_dest=None):\n",
        "    fig, m_axs = plt.subplots(2, 1, figsize=(12, 16))\n",
        "\n",
        "    m_axs[0].plot(metrics['f1_score'])\n",
        "    m_axs[0].set_ylabel('f1_score')\n",
        "    m_axs[0].set_xlabel('epoch')\n",
        "    m_axs[0].set_title('Training')\n",
        "    m_axs[0].legend(CLASS_NAMES, loc='upper left')\n",
        "\n",
        "    m_axs[1].plot(metrics['val_f1_score'])\n",
        "    m_axs[1].set_ylabel('val_f1_score')\n",
        "    m_axs[1].set_xlabel('epoch')\n",
        "    m_axs[1].set_title('Validation')\n",
        "    m_axs[1].legend(CLASS_NAMES, loc='upper left')\n",
        "\n",
        "    if save_dest:\n",
        "        fig.savefig(os.path.join(save_dest, 'f1_score.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "    plot_metric({\n",
        "        \"f1_score_average\": np.array(metrics['f1_score']).mean(axis=1),\n",
        "        \"val_f1_score_average\": np.array(metrics['val_f1_score']).mean(axis=1)},\n",
        "        'f1_score_average',\n",
        "        save_dest)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_lables, pred_labels, target_names, save_dest=None):\n",
        "    cm = confusion_matrix(true_lables, pred_labels)\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    thresh = cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    if save_dest:\n",
        "        fig.savefig(os.path.join(save_dest, 'confusion_matrix.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "# Callbacks and metrics\n",
        "# Helps to persist all metrics and weights in situation when you interupt training\n",
        "\n",
        "\n",
        "class TrainingHistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, metrics, save_weights=True, metrics_plot_dir=None):\n",
        "        self.metrics = {}\n",
        "        self.weights = []\n",
        "        self.save_weights = save_weights\n",
        "        self.metrics_plot_dir = metrics_plot_dir\n",
        "        for m in metrics:\n",
        "            self.metrics[m] = []\n",
        "            self.metrics['val_' + m] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            for key in self.metrics.keys():\n",
        "                self.metrics[key].append(logs.get(key))\n",
        "\n",
        "        if self.metrics_plot_dir:\n",
        "            for key in self.metrics.keys():\n",
        "                if not key.startswith('val_'):\n",
        "                    if key == 'f1_score':\n",
        "                        plot_f1(self.metrics, self.metrics_plot_dir)\n",
        "                    else:\n",
        "                        plot_metric(self.metrics, key, self.metrics_plot_dir)\n",
        "\n",
        "        if self.save_weights:\n",
        "            self.weights.append([])\n",
        "            for l in model.layers:\n",
        "                self.weights[len(self.weights)-1].append(l.get_weights())\n",
        "\n",
        "\n",
        "def get_callbacks(save_best_models=True, best_models_dir=None,\n",
        "                  early_stopping=True,\n",
        "                  reduce_lr_on_plateau=True,\n",
        "                  training_history=True, metrics_plot_dir=None):\n",
        "    callbacks = []\n",
        "    if save_best_models:\n",
        "        Path(best_models_dir).mkdir(parents=True, exist_ok=True)\n",
        "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(\n",
        "                best_models_dir, 'e_{epoch:02d}-acc_{val_accuracy:.2f}-f1_{val_f1_score}.hdf5'),\n",
        "            monitor='val_accuracy',\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='auto'))\n",
        "    if early_stopping:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=20,\n",
        "            restore_best_weights=True))\n",
        "    if reduce_lr_on_plateau:\n",
        "        callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.8,\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            mode='auto',\n",
        "            epsilon=0.0001,\n",
        "            cooldown=5,\n",
        "            min_lr=0.00001))\n",
        "    if training_history:\n",
        "        Path(metrics_plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "        callbacks.append(TrainingHistoryCallback(\n",
        "            ['loss', 'accuracy', 'f1_score'],\n",
        "            metrics_plot_dir=metrics_plot_dir))\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "\n",
        "def top_2_accuracy(in_gt, in_pred):\n",
        "    return tf.keras.metrics.top_k_categorical_accuracy(in_gt, in_pred, k=2)\n",
        "\n",
        "\n",
        "class FBetaScore(tf.keras.metrics.Metric):\n",
        "    \"\"\"Computes F-Beta score.\n",
        "    It is the weighted harmonic mean of precision\n",
        "    and recall. Output range is [0, 1]. Works for\n",
        "    both multi-class and multi-label classification.\n",
        "    F-Beta = (1 + beta^2) * (prec * recall) / ((beta^2 * prec) + recall)\n",
        "    Args:\n",
        "        num_classes: Number of unique classes in the dataset.\n",
        "        average: Type of averaging to be performed on data.\n",
        "            Acceptable values are `None`, `micro`, `macro` and\n",
        "            `weighted`. Default value is None.\n",
        "        beta: Determines the weight of precision and recall\n",
        "            in harmonic mean. Determines the weight given to the\n",
        "            precision and recall. Default value is 1.\n",
        "        threshold: Elements of `y_pred` greater than threshold are\n",
        "            converted to be 1, and the rest 0. If threshold is\n",
        "            None, the argmax is converted to 1, and the rest 0.\n",
        "    Returns:\n",
        "        F-Beta Score: float\n",
        "    Raises:\n",
        "        ValueError: If the `average` has values other than\n",
        "        [None, micro, macro, weighted].\n",
        "        ValueError: If the `beta` value is less than or equal\n",
        "        to 0.\n",
        "    `average` parameter behavior:\n",
        "        None: Scores for each class are returned\n",
        "        micro: True positivies, false positives and\n",
        "            false negatives are computed globally.\n",
        "        macro: True positivies, false positives and\n",
        "            false negatives are computed for each class\n",
        "            and their unweighted mean is returned.\n",
        "        weighted: Metrics are computed for each class\n",
        "            and returns the mean weighted by the\n",
        "            number of true instances in each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        average=None,\n",
        "        beta=1.0,\n",
        "        threshold=None,\n",
        "        name=\"fbeta_score\",\n",
        "        dtype=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(name=name, dtype=dtype)\n",
        "\n",
        "        if average not in (None, \"micro\", \"macro\", \"weighted\"):\n",
        "            raise ValueError(\n",
        "                \"Unknown average type. Acceptable values \"\n",
        "                \"are: [None, micro, macro, weighted]\"\n",
        "            )\n",
        "\n",
        "        if not isinstance(beta, float):\n",
        "            raise TypeError(\"The value of beta should be a python float\")\n",
        "\n",
        "        if beta <= 0.0:\n",
        "            raise ValueError(\"beta value should be greater than zero\")\n",
        "\n",
        "        if threshold is not None:\n",
        "            if not isinstance(threshold, float):\n",
        "                raise TypeError(\n",
        "                    \"The value of threshold should be a python float\")\n",
        "            if threshold > 1.0 or threshold <= 0.0:\n",
        "                raise ValueError(\"threshold should be between 0 and 1\")\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.average = average\n",
        "        self.beta = beta\n",
        "        self.threshold = threshold\n",
        "        self.axis = None\n",
        "        self.init_shape = []\n",
        "\n",
        "        if self.average != \"micro\":\n",
        "            self.axis = 0\n",
        "            self.init_shape = [self.num_classes]\n",
        "\n",
        "        def _zero_wt_init(name):\n",
        "            return self.add_weight(\n",
        "                name, shape=self.init_shape, initializer=\"zeros\", dtype=self.dtype\n",
        "            )\n",
        "\n",
        "        self.true_positives = _zero_wt_init(\"true_positives\")\n",
        "        self.false_positives = _zero_wt_init(\"false_positives\")\n",
        "        self.false_negatives = _zero_wt_init(\"false_negatives\")\n",
        "        self.weights_intermediate = _zero_wt_init(\"weights_intermediate\")\n",
        "\n",
        "    # TODO: Add sample_weight support, currently it is\n",
        "    # ignored during calculations.\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        if self.threshold is None:\n",
        "            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "            # make sure [0, 0, 0] doesn't become [1, 1, 1]\n",
        "            # Use abs(x) > eps, instead of x != 0 to check for zero\n",
        "            y_pred = tf.logical_and(\n",
        "                y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
        "        else:\n",
        "            y_pred = y_pred > self.threshold\n",
        "\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.cast(y_pred, tf.int32)\n",
        "\n",
        "        def _count_non_zero(val):\n",
        "            non_zeros = tf.math.count_nonzero(val, axis=self.axis)\n",
        "            return tf.cast(non_zeros, self.dtype)\n",
        "\n",
        "        self.true_positives.assign_add(_count_non_zero(y_pred * y_true))\n",
        "        self.false_positives.assign_add(_count_non_zero(y_pred * (y_true - 1)))\n",
        "        self.false_negatives.assign_add(_count_non_zero((y_pred - 1) * y_true))\n",
        "        self.weights_intermediate.assign_add(_count_non_zero(y_true))\n",
        "\n",
        "    def result(self):\n",
        "        precision = tf.math.divide_no_nan(\n",
        "            self.true_positives, self.true_positives + self.false_positives\n",
        "        )\n",
        "        recall = tf.math.divide_no_nan(\n",
        "            self.true_positives, self.true_positives + self.false_negatives\n",
        "        )\n",
        "\n",
        "        mul_value = precision * recall\n",
        "        add_value = (tf.math.square(self.beta) * precision) + recall\n",
        "        mean = tf.math.divide_no_nan(mul_value, add_value)\n",
        "        f1_score = mean * (1 + tf.math.square(self.beta))\n",
        "\n",
        "        if self.average == \"weighted\":\n",
        "            weights = tf.math.divide_no_nan(\n",
        "                self.weights_intermediate, tf.reduce_sum(\n",
        "                    self.weights_intermediate)\n",
        "            )\n",
        "            f1_score = tf.reduce_sum(f1_score * weights)\n",
        "\n",
        "        elif self.average is not None:  # [micro, macro]\n",
        "            f1_score = tf.reduce_mean(f1_score)\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
        "\n",
        "        config = {\n",
        "            \"num_classes\": self.num_classes,\n",
        "            \"average\": self.average,\n",
        "            \"beta\": self.beta,\n",
        "        }\n",
        "\n",
        "        if self.threshold is not None:\n",
        "            config[\"threshold\"] = self.threshold\n",
        "\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.false_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.false_negatives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.weights_intermediate.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "\n",
        "\n",
        "class F1Score(FBetaScore):\n",
        "    \"\"\"Computes F-1 Score.\n",
        "    It is the harmonic mean of precision and recall.\n",
        "    Output range is [0, 1]. Works for both multi-class\n",
        "    and multi-label classification.\n",
        "    F-1 = 2 * (precision * recall) / (precision + recall)\n",
        "    Args:\n",
        "        num_classes: Number of unique classes in the dataset.\n",
        "        average: Type of averaging to be performed on data.\n",
        "            Acceptable values are `None`, `micro`, `macro`\n",
        "            and `weighted`. Default value is None.\n",
        "        threshold: Elements of `y_pred` above threshold are\n",
        "            considered to be 1, and the rest 0. If threshold is\n",
        "            None, the argmax is converted to 1, and the rest 0.\n",
        "    Returns:\n",
        "        F-1 Score: float\n",
        "    Raises:\n",
        "        ValueError: If the `average` has values other than\n",
        "        [None, micro, macro, weighted].\n",
        "    `average` parameter behavior:\n",
        "        None: Scores for each class are returned\n",
        "        micro: True positivies, false positives and\n",
        "            false negatives are computed globally.\n",
        "        macro: True positivies, false positives and\n",
        "            false negatives are computed for each class\n",
        "            and their unweighted mean is returned.\n",
        "        weighted: Metrics are computed for each class\n",
        "            and returns the mean weighted by the\n",
        "            number of true instances in each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        average=None,\n",
        "        threshold=None,\n",
        "        name=\"f1_score\",\n",
        "        dtype=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(num_classes, average, 1.0, threshold, name=name, dtype=dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        del base_config[\"beta\"]\n",
        "        return base_config\n",
        "\n",
        "\n",
        "def get_metrics():\n",
        "    return ['accuracy', F1Score(len(CLASS_INDEXES)), top_2_accuracy]\n",
        "\n",
        "# Models\n",
        "\n",
        "\n",
        "def get_alex_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(96, (11, 11), strides=4, activation=\"relu\",  padding=\"same\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (11, 11), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(384, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(384, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), strides=1,\n",
        "                     padding=\"same\",  activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"categorical_crossentropy\", metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_custom_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                     activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation='softmax'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy', metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_vgg_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "                  metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_inception_v3(train_ds, train_steps, weights, freeze_layers_number, input_shape):\n",
        "    # create the base pre-trained model\n",
        "    base_model = InceptionV3(weights='imagenet',\n",
        "                             include_top=False,\n",
        "                             input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # first: train only the top layers (which were randomly initialized)\n",
        "    # i.e. freeze all convolutional InceptionV3 layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # compile the model (should be done *after* setting layers to non-trainable)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.001), loss='categorical_crossentropy')\n",
        "\n",
        "    # train the model on the new data for a few epochs\n",
        "    model.fit(train_ds, steps_per_epoch=train_steps,\n",
        "              epochs=3, class_weight=weights)\n",
        "\n",
        "    for layer in model.layers[:freeze_layers_number]:\n",
        "        layer.trainable = False\n",
        "    for layer in model.layers[freeze_layers_number:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # we need to recompile the model for these modifications to take effect\n",
        "    # we use SGD with a low learning rate\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_all_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (7, 7), strides=1, activation=\"relu\",  padding=\"valid\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (5, 5), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3, 3), strides=2, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3), strides=2, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3, 3), strides=2,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (1, 1), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(5, (1, 1), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"categorical_crossentropy\", metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "# Train and validation\n",
        "\n",
        "\n",
        "def train(model, train_ds, train_steps, val_ds, val_steps,\n",
        "          experiment_dir, epochs, weights=None):\n",
        "    print('Start training.')\n",
        "    history = model.fit(train_ds, steps_per_epoch=train_steps,\n",
        "                        validation_data=val_ds, validation_steps=val_steps,\n",
        "                        class_weight=weights,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=get_callbacks(\n",
        "                            save_best_models=False,\n",
        "                            best_models_dir=os.path.join(\n",
        "                                experiment_dir, 'models'),\n",
        "                            early_stopping=False,\n",
        "                            reduce_lr_on_plateau=True,\n",
        "                            training_history=True,\n",
        "                            metrics_plot_dir=experiment_dir)\n",
        "                        )\n",
        "    print('Training finished.')\n",
        "    return history\n",
        "\n",
        "\n",
        "def create_confusion_matrix(model, dataset, steps, target_names, save_dest=None):\n",
        "    print('Creating confusion matrix.')\n",
        "    it = iter(dataset)\n",
        "    true_labels_glob = []\n",
        "    pred_labels_glob = []\n",
        "\n",
        "    for i in range(0, steps):\n",
        "        image_batch, true_labels = next(it)\n",
        "        true_labels_glob.extend(np.argmax(true_labels, axis=1))\n",
        "        pred = model.predict(image_batch)\n",
        "        pred_labels_glob.extend(np.argmax(pred, axis=1))\n",
        "\n",
        "    plot_confusion_matrix(\n",
        "        true_labels_glob, pred_labels_glob, target_names, save_dest)\n",
        "    print('Confusion matrix was saved to', save_dest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUiVTTjCLOtv",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Kv5snxyRlTh",
        "outputId": "10cd5801-1eaa-4108-a318-82f5203d1d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Get args\n",
        "args = init_env()\n",
        "\n",
        "# Create input objects\n",
        "train_df, val_df, weights = prepare_data(\n",
        "    args['dataframe_path'], args['image_dir'], args['quality_dataset_path'])\n",
        "train_ds, train_count, val_ds, val_count = create_datasets(\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    img_size=args['img_size'],\n",
        "    batch_size=args['batch_size'])\n",
        "train_steps = 5000 // args['batch_size']  # train_count // args['batch_size']\n",
        "\n",
        "# Create model\n",
        "input_shape = get_input_shape(args['img_size'])\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    print('There is no model defined')\n",
        "# model = get_model(input_shape)\n",
        "model = get_vgg_model(input_shape)\n",
        "# model = get_alex_model(input_shape)\n",
        "# model = get_inception_v3(\n",
        "#     train_ds=train_ds,\n",
        "#     train_steps=train_steps,\n",
        "#     weights=weights,\n",
        "#     freeze_layers_number=172,\n",
        "#     input_shape=input_shape)\n",
        "# model = get_all_cnn_model(input_shape)\n",
        "# model.summary()\n",
        "\n",
        "# Train\n",
        "experiment_dir = os.path.join(args['experiments_dir'],\n",
        "                              datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "history = train(model=model,\n",
        "                train_ds=train_ds,\n",
        "                train_steps=train_steps,\n",
        "                val_ds=val_ds,\n",
        "                val_steps=val_count // args['batch_size'],\n",
        "                experiment_dir=experiment_dir,\n",
        "                epochs=500,\n",
        "                weights=weights)\n",
        "\n",
        "# Validate\n",
        "create_confusion_matrix(model, val_ds, val_count //\n",
        "                        args['batch_size'], CLASS_NAMES, experiment_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Arguments: {\"remote_project_dir\": \"drive/My Drive/diabetic-retinopathy-thesis\", \"project_dir\": \"/content\", \"image_dir\": \"/content/train_processed\", \"dataframe_path\": \"drive/My Drive/diabetic-retinopathy-thesis/trainLabels.csv\", \"experiments_dir\": \"drive/My Drive/diabetic-retinopathy-thesis/experiments\", \"quality_dataset_path\": null, \"gpu_id\": null, \"img_size\": 512, \"batch_size\": 16}\n",
            "Found 35126 images\n",
            "Number of existed images is 35126\n",
            "Train dataframe size: 28100 Validation dataframe size: 7026\n",
            "Weights for each level:\n",
            " {0: 1.3609066253390159, 1: 14.380757420675538, 2: 6.636750118091639, 3: 40.25787965616046, 4: 49.64664310954063}\n",
            "There is no model defined\n",
            "Start training.\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 12.4892 - accuracy: 0.2512 - f1_score: 0.1529 - top_2_accuracy: 0.4740 - val_loss: 1.3721 - val_accuracy: 0.4999 - val_f1_score: 0.1846 - val_top_2_accuracy: 0.7647 - lr: 2.0000e-05\n",
            "Epoch 2/500\n",
            "312/312 [==============================] - 261s 837ms/step - loss: 10.4925 - accuracy: 0.2594 - f1_score: 0.1670 - top_2_accuracy: 0.4772 - val_loss: 1.5879 - val_accuracy: 0.2291 - val_f1_score: 0.1364 - val_top_2_accuracy: 0.5155 - lr: 2.0000e-05\n",
            "Epoch 3/500\n",
            "312/312 [==============================] - 262s 839ms/step - loss: 9.9169 - accuracy: 0.2280 - f1_score: 0.1555 - top_2_accuracy: 0.4555 - val_loss: 1.5434 - val_accuracy: 0.2121 - val_f1_score: 0.1536 - val_top_2_accuracy: 0.4981 - lr: 2.0000e-05\n",
            "Epoch 4/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 9.2655 - accuracy: 0.2019 - f1_score: 0.1468 - top_2_accuracy: 0.4265 - val_loss: 1.5715 - val_accuracy: 0.1990 - val_f1_score: 0.1512 - val_top_2_accuracy: 0.4821 - lr: 2.0000e-05\n",
            "Epoch 5/500\n",
            "312/312 [==============================] - 263s 842ms/step - loss: 8.5417 - accuracy: 0.2113 - f1_score: 0.1473 - top_2_accuracy: 0.4339 - val_loss: 1.4201 - val_accuracy: 0.4149 - val_f1_score: 0.2107 - val_top_2_accuracy: 0.7037 - lr: 2.0000e-05\n",
            "Epoch 6/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 8.4007 - accuracy: 0.2280 - f1_score: 0.1549 - top_2_accuracy: 0.4473 - val_loss: 1.4989 - val_accuracy: 0.2913 - val_f1_score: 0.1767 - val_top_2_accuracy: 0.5896 - lr: 2.0000e-05\n",
            "Epoch 7/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 8.3912 - accuracy: 0.2085 - f1_score: 0.1449 - top_2_accuracy: 0.4209 - val_loss: 1.5731 - val_accuracy: 0.1953 - val_f1_score: 0.1501 - val_top_2_accuracy: 0.4846 - lr: 2.0000e-05\n",
            "Epoch 8/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 8.5045 - accuracy: 0.2161 - f1_score: 0.1451 - top_2_accuracy: 0.4461 - val_loss: 1.5522 - val_accuracy: 0.1969 - val_f1_score: 0.1439 - val_top_2_accuracy: 0.5157 - lr: 2.0000e-05\n",
            "Epoch 9/500\n",
            "312/312 [==============================] - 266s 851ms/step - loss: 7.8657 - accuracy: 0.2065 - f1_score: 0.1423 - top_2_accuracy: 0.4297 - val_loss: 1.6128 - val_accuracy: 0.2194 - val_f1_score: 0.1542 - val_top_2_accuracy: 0.4389 - lr: 2.0000e-05\n",
            "Epoch 10/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 8.4357 - accuracy: 0.1905 - f1_score: 0.1413 - top_2_accuracy: 0.4040 - val_loss: 1.5687 - val_accuracy: 0.1878 - val_f1_score: 0.1414 - val_top_2_accuracy: 0.4680 - lr: 2.0000e-05\n",
            "Epoch 11/500\n",
            "312/312 [==============================] - ETA: 0s - loss: 8.1376 - accuracy: 0.2123 - f1_score: 0.1472 - top_2_accuracy: 0.4287\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 8.1376 - accuracy: 0.2123 - f1_score: 0.1472 - top_2_accuracy: 0.4287 - val_loss: 1.6527 - val_accuracy: 0.1619 - val_f1_score: 0.1274 - val_top_2_accuracy: 0.4016 - lr: 2.0000e-05\n",
            "Epoch 12/500\n",
            "312/312 [==============================] - 269s 861ms/step - loss: 7.8330 - accuracy: 0.2175 - f1_score: 0.1456 - top_2_accuracy: 0.4535 - val_loss: 1.4945 - val_accuracy: 0.2715 - val_f1_score: 0.1727 - val_top_2_accuracy: 0.6014 - lr: 1.6000e-05\n",
            "Epoch 13/500\n",
            "312/312 [==============================] - 264s 847ms/step - loss: 7.8859 - accuracy: 0.2185 - f1_score: 0.1465 - top_2_accuracy: 0.4527 - val_loss: 1.5617 - val_accuracy: 0.2651 - val_f1_score: 0.1516 - val_top_2_accuracy: 0.5729 - lr: 1.6000e-05\n",
            "Epoch 14/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 8.1289 - accuracy: 0.2109 - f1_score: 0.1473 - top_2_accuracy: 0.4445 - val_loss: 1.5259 - val_accuracy: 0.2154 - val_f1_score: 0.1642 - val_top_2_accuracy: 0.5275 - lr: 1.6000e-05\n",
            "Epoch 15/500\n",
            "312/312 [==============================] - 263s 843ms/step - loss: 7.9416 - accuracy: 0.2418 - f1_score: 0.1641 - top_2_accuracy: 0.4772 - val_loss: 1.4922 - val_accuracy: 0.3256 - val_f1_score: 0.1780 - val_top_2_accuracy: 0.5883 - lr: 1.6000e-05\n",
            "Epoch 16/500\n",
            "312/312 [==============================] - 262s 841ms/step - loss: 8.1491 - accuracy: 0.2015 - f1_score: 0.1462 - top_2_accuracy: 0.4265 - val_loss: 1.4575 - val_accuracy: 0.2355 - val_f1_score: 0.1881 - val_top_2_accuracy: 0.6530 - lr: 1.6000e-05\n",
            "Epoch 17/500\n",
            "312/312 [==============================] - 263s 843ms/step - loss: 7.5796 - accuracy: 0.2714 - f1_score: 0.1715 - top_2_accuracy: 0.5146 - val_loss: 1.4762 - val_accuracy: 0.4079 - val_f1_score: 0.2150 - val_top_2_accuracy: 0.6901 - lr: 1.6000e-05\n",
            "Epoch 18/500\n",
            "312/312 [==============================] - 265s 849ms/step - loss: 7.9197 - accuracy: 0.2452 - f1_score: 0.1613 - top_2_accuracy: 0.4862 - val_loss: 1.4358 - val_accuracy: 0.3542 - val_f1_score: 0.1830 - val_top_2_accuracy: 0.6999 - lr: 1.6000e-05\n",
            "Epoch 19/500\n",
            "312/312 [==============================] - 263s 845ms/step - loss: 7.8034 - accuracy: 0.2376 - f1_score: 0.1580 - top_2_accuracy: 0.4884 - val_loss: 1.5666 - val_accuracy: 0.1888 - val_f1_score: 0.1312 - val_top_2_accuracy: 0.5745 - lr: 1.6000e-05\n",
            "Epoch 20/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 7.5156 - accuracy: 0.2812 - f1_score: 0.1769 - top_2_accuracy: 0.5539 - val_loss: 1.3889 - val_accuracy: 0.3498 - val_f1_score: 0.1979 - val_top_2_accuracy: 0.7079 - lr: 1.6000e-05\n",
            "Epoch 21/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 8.1996 - accuracy: 0.2528 - f1_score: 0.1729 - top_2_accuracy: 0.5050 - val_loss: 1.5233 - val_accuracy: 0.2863 - val_f1_score: 0.1774 - val_top_2_accuracy: 0.5897 - lr: 1.6000e-05\n",
            "Epoch 22/500\n",
            "312/312 [==============================] - 265s 851ms/step - loss: 7.6783 - accuracy: 0.2770 - f1_score: 0.1774 - top_2_accuracy: 0.5341 - val_loss: 1.3210 - val_accuracy: 0.4428 - val_f1_score: 0.2358 - val_top_2_accuracy: 0.7580 - lr: 1.6000e-05\n",
            "Epoch 23/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 7.6366 - accuracy: 0.2668 - f1_score: 0.1763 - top_2_accuracy: 0.5210 - val_loss: 1.4662 - val_accuracy: 0.3118 - val_f1_score: 0.2064 - val_top_2_accuracy: 0.6660 - lr: 1.6000e-05\n",
            "Epoch 24/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 7.5888 - accuracy: 0.2788 - f1_score: 0.1790 - top_2_accuracy: 0.5310 - val_loss: 1.4317 - val_accuracy: 0.4683 - val_f1_score: 0.2326 - val_top_2_accuracy: 0.6918 - lr: 1.6000e-05\n",
            "Epoch 25/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 7.9693 - accuracy: 0.3007 - f1_score: 0.1838 - top_2_accuracy: 0.5783 - val_loss: 1.3860 - val_accuracy: 0.4981 - val_f1_score: 0.2573 - val_top_2_accuracy: 0.7247 - lr: 1.6000e-05\n",
            "Epoch 26/500\n",
            "312/312 [==============================] - 264s 847ms/step - loss: 7.5738 - accuracy: 0.2821 - f1_score: 0.1800 - top_2_accuracy: 0.5635 - val_loss: 1.5081 - val_accuracy: 0.3667 - val_f1_score: 0.1904 - val_top_2_accuracy: 0.6365 - lr: 1.6000e-05\n",
            "Epoch 27/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 7.8772 - accuracy: 0.2889 - f1_score: 0.1934 - top_2_accuracy: 0.5687 - val_loss: 1.3771 - val_accuracy: 0.4788 - val_f1_score: 0.2408 - val_top_2_accuracy: 0.7360 - lr: 1.6000e-05\n",
            "Epoch 28/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 7.4379 - accuracy: 0.2963 - f1_score: 0.1926 - top_2_accuracy: 0.5545 - val_loss: 1.5278 - val_accuracy: 0.4557 - val_f1_score: 0.2240 - val_top_2_accuracy: 0.6649 - lr: 1.6000e-05\n",
            "Epoch 29/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 7.3754 - accuracy: 0.2973 - f1_score: 0.1962 - top_2_accuracy: 0.5661 - val_loss: 1.4745 - val_accuracy: 0.3259 - val_f1_score: 0.2184 - val_top_2_accuracy: 0.6076 - lr: 1.6000e-05\n",
            "Epoch 30/500\n",
            "312/312 [==============================] - 263s 842ms/step - loss: 7.3674 - accuracy: 0.2943 - f1_score: 0.1966 - top_2_accuracy: 0.5661 - val_loss: 1.2868 - val_accuracy: 0.5900 - val_f1_score: 0.2813 - val_top_2_accuracy: 0.7864 - lr: 1.6000e-05\n",
            "Epoch 31/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 7.6480 - accuracy: 0.3001 - f1_score: 0.1926 - top_2_accuracy: 0.5893 - val_loss: 1.1632 - val_accuracy: 0.6536 - val_f1_score: 0.2707 - val_top_2_accuracy: 0.8062 - lr: 1.6000e-05\n",
            "Epoch 32/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 7.7689 - accuracy: 0.2893 - f1_score: 0.2020 - top_2_accuracy: 0.5703 - val_loss: 1.4626 - val_accuracy: 0.2745 - val_f1_score: 0.2052 - val_top_2_accuracy: 0.6775 - lr: 1.6000e-05\n",
            "Epoch 33/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 7.5487 - accuracy: 0.2901 - f1_score: 0.1901 - top_2_accuracy: 0.5771 - val_loss: 1.4303 - val_accuracy: 0.3391 - val_f1_score: 0.2059 - val_top_2_accuracy: 0.6501 - lr: 1.6000e-05\n",
            "Epoch 34/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 7.3975 - accuracy: 0.2849 - f1_score: 0.1923 - top_2_accuracy: 0.5533 - val_loss: 1.3815 - val_accuracy: 0.4120 - val_f1_score: 0.2320 - val_top_2_accuracy: 0.7567 - lr: 1.6000e-05\n",
            "Epoch 35/500\n",
            "312/312 [==============================] - 262s 838ms/step - loss: 7.3382 - accuracy: 0.2993 - f1_score: 0.1971 - top_2_accuracy: 0.5767 - val_loss: 1.3033 - val_accuracy: 0.3965 - val_f1_score: 0.2297 - val_top_2_accuracy: 0.7600 - lr: 1.6000e-05\n",
            "Epoch 36/500\n",
            "312/312 [==============================] - 262s 838ms/step - loss: 7.4945 - accuracy: 0.2971 - f1_score: 0.1894 - top_2_accuracy: 0.6000 - val_loss: 1.4224 - val_accuracy: 0.2761 - val_f1_score: 0.1803 - val_top_2_accuracy: 0.6597 - lr: 1.6000e-05\n",
            "Epoch 37/500\n",
            "312/312 [==============================] - 262s 838ms/step - loss: 7.1038 - accuracy: 0.3129 - f1_score: 0.1954 - top_2_accuracy: 0.5986 - val_loss: 1.2734 - val_accuracy: 0.4953 - val_f1_score: 0.2731 - val_top_2_accuracy: 0.7930 - lr: 1.6000e-05\n",
            "Epoch 38/500\n",
            "312/312 [==============================] - 261s 838ms/step - loss: 7.8629 - accuracy: 0.3001 - f1_score: 0.2049 - top_2_accuracy: 0.5777 - val_loss: 1.3188 - val_accuracy: 0.4226 - val_f1_score: 0.2240 - val_top_2_accuracy: 0.7486 - lr: 1.6000e-05\n",
            "Epoch 39/500\n",
            "312/312 [==============================] - 262s 839ms/step - loss: 7.2766 - accuracy: 0.3219 - f1_score: 0.2071 - top_2_accuracy: 0.5998 - val_loss: 1.2562 - val_accuracy: 0.6150 - val_f1_score: 0.2896 - val_top_2_accuracy: 0.8074 - lr: 1.6000e-05\n",
            "Epoch 40/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 7.2616 - accuracy: 0.3067 - f1_score: 0.1993 - top_2_accuracy: 0.5831 - val_loss: 1.2479 - val_accuracy: 0.5552 - val_f1_score: 0.2668 - val_top_2_accuracy: 0.7601 - lr: 1.6000e-05\n",
            "Epoch 41/500\n",
            "312/312 [==============================] - ETA: 0s - loss: 7.2675 - accuracy: 0.3357 - f1_score: 0.2173 - top_2_accuracy: 0.6222\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.2799999967683107e-05.\n",
            "312/312 [==============================] - 265s 849ms/step - loss: 7.2675 - accuracy: 0.3357 - f1_score: 0.2173 - top_2_accuracy: 0.6222 - val_loss: 1.1858 - val_accuracy: 0.6411 - val_f1_score: 0.2652 - val_top_2_accuracy: 0.7637 - lr: 1.6000e-05\n",
            "Epoch 42/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 7.4955 - accuracy: 0.3035 - f1_score: 0.2068 - top_2_accuracy: 0.5831 - val_loss: 1.1505 - val_accuracy: 0.6334 - val_f1_score: 0.3099 - val_top_2_accuracy: 0.7997 - lr: 1.2800e-05\n",
            "Epoch 43/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 6.9993 - accuracy: 0.3079 - f1_score: 0.2085 - top_2_accuracy: 0.6028 - val_loss: 1.3409 - val_accuracy: 0.3767 - val_f1_score: 0.2550 - val_top_2_accuracy: 0.7150 - lr: 1.2800e-05\n",
            "Epoch 44/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 7.3848 - accuracy: 0.2632 - f1_score: 0.1946 - top_2_accuracy: 0.5467 - val_loss: 1.2998 - val_accuracy: 0.4805 - val_f1_score: 0.2562 - val_top_2_accuracy: 0.7437 - lr: 1.2800e-05\n",
            "Epoch 45/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 6.9001 - accuracy: 0.3187 - f1_score: 0.2138 - top_2_accuracy: 0.5964 - val_loss: 1.3013 - val_accuracy: 0.5652 - val_f1_score: 0.2612 - val_top_2_accuracy: 0.7533 - lr: 1.2800e-05\n",
            "Epoch 46/500\n",
            "312/312 [==============================] - 267s 856ms/step - loss: 6.9597 - accuracy: 0.3229 - f1_score: 0.2132 - top_2_accuracy: 0.6118 - val_loss: 1.2205 - val_accuracy: 0.5184 - val_f1_score: 0.2758 - val_top_2_accuracy: 0.7452 - lr: 1.2800e-05\n",
            "Epoch 47/500\n",
            "312/312 [==============================] - 263s 842ms/step - loss: 7.1127 - accuracy: 0.3283 - f1_score: 0.2165 - top_2_accuracy: 0.6014 - val_loss: 1.4127 - val_accuracy: 0.4480 - val_f1_score: 0.2333 - val_top_2_accuracy: 0.6233 - lr: 1.2800e-05\n",
            "Epoch 48/500\n",
            "312/312 [==============================] - 264s 848ms/step - loss: 7.1632 - accuracy: 0.3355 - f1_score: 0.2258 - top_2_accuracy: 0.6164 - val_loss: 1.3320 - val_accuracy: 0.3253 - val_f1_score: 0.2196 - val_top_2_accuracy: 0.7632 - lr: 1.2800e-05\n",
            "Epoch 49/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 7.2245 - accuracy: 0.2963 - f1_score: 0.2125 - top_2_accuracy: 0.6102 - val_loss: 1.3217 - val_accuracy: 0.3158 - val_f1_score: 0.2158 - val_top_2_accuracy: 0.7595 - lr: 1.2800e-05\n",
            "Epoch 50/500\n",
            "312/312 [==============================] - 266s 851ms/step - loss: 7.0166 - accuracy: 0.3081 - f1_score: 0.2154 - top_2_accuracy: 0.6038 - val_loss: 1.2745 - val_accuracy: 0.5671 - val_f1_score: 0.2597 - val_top_2_accuracy: 0.7474 - lr: 1.2800e-05\n",
            "Epoch 51/500\n",
            "312/312 [==============================] - 270s 864ms/step - loss: 6.9593 - accuracy: 0.3169 - f1_score: 0.2104 - top_2_accuracy: 0.6102 - val_loss: 1.2609 - val_accuracy: 0.4808 - val_f1_score: 0.2796 - val_top_2_accuracy: 0.7674 - lr: 1.2800e-05\n",
            "Epoch 52/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.9495 - accuracy: 0.3279 - f1_score: 0.2227 - top_2_accuracy: 0.6240 - val_loss: 1.2711 - val_accuracy: 0.5118 - val_f1_score: 0.2655 - val_top_2_accuracy: 0.7272 - lr: 1.2800e-05\n",
            "Epoch 53/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 7.1985 - accuracy: 0.3145 - f1_score: 0.2246 - top_2_accuracy: 0.6116 - val_loss: 1.5407 - val_accuracy: 0.3125 - val_f1_score: 0.1993 - val_top_2_accuracy: 0.5588 - lr: 1.2800e-05\n",
            "Epoch 54/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 6.7170 - accuracy: 0.3309 - f1_score: 0.2164 - top_2_accuracy: 0.6288 - val_loss: 1.3130 - val_accuracy: 0.5224 - val_f1_score: 0.2592 - val_top_2_accuracy: 0.7110 - lr: 1.2800e-05\n",
            "Epoch 55/500\n",
            "312/312 [==============================] - ETA: 0s - loss: 7.3780 - accuracy: 0.3165 - f1_score: 0.2316 - top_2_accuracy: 0.5853\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0240000119665639e-05.\n",
            "312/312 [==============================] - 265s 851ms/step - loss: 7.3780 - accuracy: 0.3165 - f1_score: 0.2316 - top_2_accuracy: 0.5853 - val_loss: 1.2621 - val_accuracy: 0.5224 - val_f1_score: 0.2961 - val_top_2_accuracy: 0.7671 - lr: 1.2800e-05\n",
            "Epoch 56/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 6.6951 - accuracy: 0.3111 - f1_score: 0.2334 - top_2_accuracy: 0.6050 - val_loss: 1.2523 - val_accuracy: 0.4902 - val_f1_score: 0.2875 - val_top_2_accuracy: 0.7184 - lr: 1.0240e-05\n",
            "Epoch 57/500\n",
            "312/312 [==============================] - 266s 851ms/step - loss: 6.8677 - accuracy: 0.3291 - f1_score: 0.2269 - top_2_accuracy: 0.6098 - val_loss: 1.2361 - val_accuracy: 0.5243 - val_f1_score: 0.2927 - val_top_2_accuracy: 0.7647 - lr: 1.0240e-05\n",
            "Epoch 58/500\n",
            "312/312 [==============================] - 264s 847ms/step - loss: 6.9164 - accuracy: 0.3245 - f1_score: 0.2299 - top_2_accuracy: 0.6100 - val_loss: 1.3710 - val_accuracy: 0.4658 - val_f1_score: 0.2595 - val_top_2_accuracy: 0.6903 - lr: 1.0240e-05\n",
            "Epoch 59/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 7.1412 - accuracy: 0.3273 - f1_score: 0.2239 - top_2_accuracy: 0.6184 - val_loss: 1.3785 - val_accuracy: 0.4347 - val_f1_score: 0.2599 - val_top_2_accuracy: 0.6905 - lr: 1.0240e-05\n",
            "Epoch 60/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 6.9750 - accuracy: 0.3155 - f1_score: 0.2251 - top_2_accuracy: 0.6284 - val_loss: 1.3345 - val_accuracy: 0.3386 - val_f1_score: 0.2313 - val_top_2_accuracy: 0.7234 - lr: 1.0240e-05\n",
            "Epoch 61/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 6.8991 - accuracy: 0.3065 - f1_score: 0.2267 - top_2_accuracy: 0.6162 - val_loss: 1.1139 - val_accuracy: 0.6454 - val_f1_score: 0.3226 - val_top_2_accuracy: 0.7893 - lr: 1.0240e-05\n",
            "Epoch 62/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 6.7047 - accuracy: 0.3566 - f1_score: 0.2337 - top_2_accuracy: 0.6360 - val_loss: 1.0669 - val_accuracy: 0.6808 - val_f1_score: 0.3224 - val_top_2_accuracy: 0.8155 - lr: 1.0240e-05\n",
            "Epoch 63/500\n",
            "312/312 [==============================] - 265s 849ms/step - loss: 6.8827 - accuracy: 0.3524 - f1_score: 0.2306 - top_2_accuracy: 0.6464 - val_loss: 1.2915 - val_accuracy: 0.4933 - val_f1_score: 0.2599 - val_top_2_accuracy: 0.7124 - lr: 1.0240e-05\n",
            "Epoch 64/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 7.0289 - accuracy: 0.3391 - f1_score: 0.2338 - top_2_accuracy: 0.6514 - val_loss: 1.4713 - val_accuracy: 0.3071 - val_f1_score: 0.2076 - val_top_2_accuracy: 0.6276 - lr: 1.0240e-05\n",
            "Epoch 65/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 6.7691 - accuracy: 0.3472 - f1_score: 0.2301 - top_2_accuracy: 0.6593 - val_loss: 1.2858 - val_accuracy: 0.3851 - val_f1_score: 0.2674 - val_top_2_accuracy: 0.7302 - lr: 1.0240e-05\n",
            "Epoch 66/500\n",
            "312/312 [==============================] - 265s 851ms/step - loss: 7.0918 - accuracy: 0.3149 - f1_score: 0.2388 - top_2_accuracy: 0.6168 - val_loss: 1.2849 - val_accuracy: 0.4946 - val_f1_score: 0.2755 - val_top_2_accuracy: 0.7294 - lr: 1.0240e-05\n",
            "Epoch 67/500\n",
            "312/312 [==============================] - 266s 851ms/step - loss: 6.6483 - accuracy: 0.3327 - f1_score: 0.2392 - top_2_accuracy: 0.6406 - val_loss: 1.2048 - val_accuracy: 0.6230 - val_f1_score: 0.2998 - val_top_2_accuracy: 0.7899 - lr: 1.0240e-05\n",
            "Epoch 68/500\n",
            "312/312 [==============================] - 269s 861ms/step - loss: 6.6719 - accuracy: 0.3660 - f1_score: 0.2376 - top_2_accuracy: 0.6581 - val_loss: 1.2213 - val_accuracy: 0.5802 - val_f1_score: 0.3066 - val_top_2_accuracy: 0.7761 - lr: 1.0240e-05\n",
            "Epoch 69/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 6.5982 - accuracy: 0.3534 - f1_score: 0.2505 - top_2_accuracy: 0.6542 - val_loss: 1.2104 - val_accuracy: 0.5645 - val_f1_score: 0.3019 - val_top_2_accuracy: 0.7494 - lr: 1.0240e-05\n",
            "Epoch 70/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.8058 - accuracy: 0.3448 - f1_score: 0.2422 - top_2_accuracy: 0.6567 - val_loss: 1.2343 - val_accuracy: 0.6194 - val_f1_score: 0.2772 - val_top_2_accuracy: 0.7617 - lr: 1.0240e-05\n",
            "Epoch 71/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.6159 - accuracy: 0.3518 - f1_score: 0.2457 - top_2_accuracy: 0.6560 - val_loss: 1.2063 - val_accuracy: 0.5421 - val_f1_score: 0.3122 - val_top_2_accuracy: 0.7694 - lr: 1.0240e-05\n",
            "Epoch 72/500\n",
            "312/312 [==============================] - ETA: 0s - loss: 7.1022 - accuracy: 0.3297 - f1_score: 0.2494 - top_2_accuracy: 0.6318\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "312/312 [==============================] - 266s 854ms/step - loss: 7.1022 - accuracy: 0.3297 - f1_score: 0.2494 - top_2_accuracy: 0.6318 - val_loss: 1.3259 - val_accuracy: 0.4174 - val_f1_score: 0.2412 - val_top_2_accuracy: 0.7188 - lr: 1.0240e-05\n",
            "Epoch 73/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 6.4933 - accuracy: 0.3353 - f1_score: 0.2410 - top_2_accuracy: 0.6450 - val_loss: 1.2045 - val_accuracy: 0.6360 - val_f1_score: 0.3165 - val_top_2_accuracy: 0.8017 - lr: 1.0000e-05\n",
            "Epoch 74/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 6.5514 - accuracy: 0.3980 - f1_score: 0.2602 - top_2_accuracy: 0.6767 - val_loss: 1.2938 - val_accuracy: 0.5357 - val_f1_score: 0.2828 - val_top_2_accuracy: 0.7318 - lr: 1.0000e-05\n",
            "Epoch 75/500\n",
            "312/312 [==============================] - 263s 844ms/step - loss: 6.6250 - accuracy: 0.3596 - f1_score: 0.2490 - top_2_accuracy: 0.6675 - val_loss: 1.1425 - val_accuracy: 0.5787 - val_f1_score: 0.3254 - val_top_2_accuracy: 0.8212 - lr: 1.0000e-05\n",
            "Epoch 76/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 6.8638 - accuracy: 0.3209 - f1_score: 0.2314 - top_2_accuracy: 0.6613 - val_loss: 1.2228 - val_accuracy: 0.5756 - val_f1_score: 0.2936 - val_top_2_accuracy: 0.7948 - lr: 1.0000e-05\n",
            "Epoch 77/500\n",
            "312/312 [==============================] - 268s 858ms/step - loss: 6.8069 - accuracy: 0.3560 - f1_score: 0.2602 - top_2_accuracy: 0.6761 - val_loss: 1.2199 - val_accuracy: 0.4749 - val_f1_score: 0.3012 - val_top_2_accuracy: 0.7743 - lr: 1.0000e-05\n",
            "Epoch 78/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 6.5182 - accuracy: 0.3361 - f1_score: 0.2514 - top_2_accuracy: 0.6392 - val_loss: 1.1734 - val_accuracy: 0.5958 - val_f1_score: 0.3052 - val_top_2_accuracy: 0.7655 - lr: 1.0000e-05\n",
            "Epoch 79/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 6.3803 - accuracy: 0.3662 - f1_score: 0.2498 - top_2_accuracy: 0.6637 - val_loss: 1.1589 - val_accuracy: 0.5934 - val_f1_score: 0.3277 - val_top_2_accuracy: 0.7997 - lr: 1.0000e-05\n",
            "Epoch 80/500\n",
            "312/312 [==============================] - 264s 845ms/step - loss: 6.7374 - accuracy: 0.3710 - f1_score: 0.2634 - top_2_accuracy: 0.6460 - val_loss: 1.4544 - val_accuracy: 0.4174 - val_f1_score: 0.2482 - val_top_2_accuracy: 0.6038 - lr: 1.0000e-05\n",
            "Epoch 81/500\n",
            "312/312 [==============================] - 265s 850ms/step - loss: 6.6459 - accuracy: 0.3570 - f1_score: 0.2486 - top_2_accuracy: 0.6605 - val_loss: 1.3602 - val_accuracy: 0.3911 - val_f1_score: 0.2474 - val_top_2_accuracy: 0.6878 - lr: 1.0000e-05\n",
            "Epoch 82/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 6.4392 - accuracy: 0.3317 - f1_score: 0.2340 - top_2_accuracy: 0.6623 - val_loss: 1.2551 - val_accuracy: 0.4458 - val_f1_score: 0.3053 - val_top_2_accuracy: 0.7477 - lr: 1.0000e-05\n",
            "Epoch 83/500\n",
            "312/312 [==============================] - 265s 851ms/step - loss: 7.0104 - accuracy: 0.3291 - f1_score: 0.2523 - top_2_accuracy: 0.6567 - val_loss: 1.3732 - val_accuracy: 0.4812 - val_f1_score: 0.2707 - val_top_2_accuracy: 0.6861 - lr: 1.0000e-05\n",
            "Epoch 84/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.2304 - accuracy: 0.3427 - f1_score: 0.2456 - top_2_accuracy: 0.6569 - val_loss: 1.3130 - val_accuracy: 0.4806 - val_f1_score: 0.2945 - val_top_2_accuracy: 0.7079 - lr: 1.0000e-05\n",
            "Epoch 85/500\n",
            "312/312 [==============================] - 269s 862ms/step - loss: 6.4541 - accuracy: 0.3598 - f1_score: 0.2572 - top_2_accuracy: 0.6434 - val_loss: 1.2075 - val_accuracy: 0.5384 - val_f1_score: 0.3230 - val_top_2_accuracy: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 86/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 6.5892 - accuracy: 0.3608 - f1_score: 0.2479 - top_2_accuracy: 0.6603 - val_loss: 1.1624 - val_accuracy: 0.4732 - val_f1_score: 0.3156 - val_top_2_accuracy: 0.7560 - lr: 1.0000e-05\n",
            "Epoch 87/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.8007 - accuracy: 0.3468 - f1_score: 0.2526 - top_2_accuracy: 0.6426 - val_loss: 1.1438 - val_accuracy: 0.5769 - val_f1_score: 0.3350 - val_top_2_accuracy: 0.8098 - lr: 1.0000e-05\n",
            "Epoch 88/500\n",
            "312/312 [==============================] - 263s 843ms/step - loss: 6.4954 - accuracy: 0.3776 - f1_score: 0.2655 - top_2_accuracy: 0.6855 - val_loss: 1.1921 - val_accuracy: 0.5272 - val_f1_score: 0.3053 - val_top_2_accuracy: 0.7803 - lr: 1.0000e-05\n",
            "Epoch 89/500\n",
            "312/312 [==============================] - 264s 846ms/step - loss: 6.6705 - accuracy: 0.3498 - f1_score: 0.2530 - top_2_accuracy: 0.6677 - val_loss: 1.2223 - val_accuracy: 0.5105 - val_f1_score: 0.2942 - val_top_2_accuracy: 0.7719 - lr: 1.0000e-05\n",
            "Epoch 90/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 6.4468 - accuracy: 0.3415 - f1_score: 0.2581 - top_2_accuracy: 0.6575 - val_loss: 1.1072 - val_accuracy: 0.5181 - val_f1_score: 0.3348 - val_top_2_accuracy: 0.8159 - lr: 1.0000e-05\n",
            "Epoch 91/500\n",
            "312/312 [==============================] - 266s 852ms/step - loss: 6.2874 - accuracy: 0.3602 - f1_score: 0.2519 - top_2_accuracy: 0.6765 - val_loss: 1.3316 - val_accuracy: 0.4497 - val_f1_score: 0.2845 - val_top_2_accuracy: 0.6666 - lr: 1.0000e-05\n",
            "Epoch 92/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 6.5540 - accuracy: 0.3636 - f1_score: 0.2594 - top_2_accuracy: 0.6765 - val_loss: 1.1658 - val_accuracy: 0.5467 - val_f1_score: 0.3314 - val_top_2_accuracy: 0.7948 - lr: 1.0000e-05\n",
            "Epoch 93/500\n",
            "312/312 [==============================] - 265s 849ms/step - loss: 6.5214 - accuracy: 0.3750 - f1_score: 0.2690 - top_2_accuracy: 0.6803 - val_loss: 1.1641 - val_accuracy: 0.5457 - val_f1_score: 0.3341 - val_top_2_accuracy: 0.8095 - lr: 1.0000e-05\n",
            "Epoch 94/500\n",
            "312/312 [==============================] - 264s 847ms/step - loss: 6.6089 - accuracy: 0.3433 - f1_score: 0.2567 - top_2_accuracy: 0.6701 - val_loss: 1.2059 - val_accuracy: 0.5026 - val_f1_score: 0.3094 - val_top_2_accuracy: 0.7783 - lr: 1.0000e-05\n",
            "Epoch 95/500\n",
            "312/312 [==============================] - 263s 843ms/step - loss: 6.2975 - accuracy: 0.3762 - f1_score: 0.2722 - top_2_accuracy: 0.6799 - val_loss: 1.2634 - val_accuracy: 0.4536 - val_f1_score: 0.2917 - val_top_2_accuracy: 0.7551 - lr: 1.0000e-05\n",
            "Epoch 96/500\n",
            "312/312 [==============================] - 265s 848ms/step - loss: 6.4190 - accuracy: 0.3842 - f1_score: 0.2685 - top_2_accuracy: 0.6929 - val_loss: 1.3155 - val_accuracy: 0.4069 - val_f1_score: 0.2681 - val_top_2_accuracy: 0.7181 - lr: 1.0000e-05\n",
            "Epoch 97/500\n",
            "312/312 [==============================] - 263s 843ms/step - loss: 6.5386 - accuracy: 0.3568 - f1_score: 0.2628 - top_2_accuracy: 0.6735 - val_loss: 1.1842 - val_accuracy: 0.5034 - val_f1_score: 0.3103 - val_top_2_accuracy: 0.7970 - lr: 1.0000e-05\n",
            "Epoch 98/500\n",
            "312/312 [==============================] - 261s 838ms/step - loss: 6.6099 - accuracy: 0.3590 - f1_score: 0.2514 - top_2_accuracy: 0.6997 - val_loss: 1.2477 - val_accuracy: 0.4458 - val_f1_score: 0.2766 - val_top_2_accuracy: 0.7571 - lr: 1.0000e-05\n",
            "Epoch 99/500\n",
            "312/312 [==============================] - 261s 836ms/step - loss: 6.1814 - accuracy: 0.3706 - f1_score: 0.2627 - top_2_accuracy: 0.7079 - val_loss: 1.2964 - val_accuracy: 0.3788 - val_f1_score: 0.2562 - val_top_2_accuracy: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 100/500\n",
            "312/312 [==============================] - 261s 838ms/step - loss: 6.8770 - accuracy: 0.3638 - f1_score: 0.2670 - top_2_accuracy: 0.6653 - val_loss: 1.2371 - val_accuracy: 0.5567 - val_f1_score: 0.3032 - val_top_2_accuracy: 0.7614 - lr: 1.0000e-05\n",
            "Epoch 101/500\n",
            "312/312 [==============================] - 261s 836ms/step - loss: 6.2729 - accuracy: 0.3858 - f1_score: 0.2758 - top_2_accuracy: 0.6965 - val_loss: 1.1685 - val_accuracy: 0.5399 - val_f1_score: 0.3454 - val_top_2_accuracy: 0.7957 - lr: 1.0000e-05\n",
            "Epoch 102/500\n",
            "312/312 [==============================] - 261s 837ms/step - loss: 6.2480 - accuracy: 0.3834 - f1_score: 0.2640 - top_2_accuracy: 0.6823 - val_loss: 1.0846 - val_accuracy: 0.6247 - val_f1_score: 0.3494 - val_top_2_accuracy: 0.8109 - lr: 1.0000e-05\n",
            "Epoch 103/500\n",
            "312/312 [==============================] - 261s 836ms/step - loss: 6.3844 - accuracy: 0.3704 - f1_score: 0.2611 - top_2_accuracy: 0.6715 - val_loss: 1.0893 - val_accuracy: 0.6032 - val_f1_score: 0.3458 - val_top_2_accuracy: 0.8284 - lr: 1.0000e-05\n",
            "Epoch 104/500\n",
            "312/312 [==============================] - 261s 836ms/step - loss: 6.6881 - accuracy: 0.3550 - f1_score: 0.2578 - top_2_accuracy: 0.6709 - val_loss: 1.1557 - val_accuracy: 0.5773 - val_f1_score: 0.3439 - val_top_2_accuracy: 0.8052 - lr: 1.0000e-05\n",
            "Epoch 105/500\n",
            "312/312 [==============================] - 260s 834ms/step - loss: 6.1685 - accuracy: 0.3898 - f1_score: 0.2821 - top_2_accuracy: 0.7147 - val_loss: 1.1098 - val_accuracy: 0.5595 - val_f1_score: 0.3494 - val_top_2_accuracy: 0.8185 - lr: 1.0000e-05\n",
            "Epoch 106/500\n",
            "312/312 [==============================] - 260s 832ms/step - loss: 6.2864 - accuracy: 0.3550 - f1_score: 0.2772 - top_2_accuracy: 0.6749 - val_loss: 1.2226 - val_accuracy: 0.4226 - val_f1_score: 0.2902 - val_top_2_accuracy: 0.7745 - lr: 1.0000e-05\n",
            "Epoch 107/500\n",
            "312/312 [==============================] - 261s 836ms/step - loss: 6.1965 - accuracy: 0.3690 - f1_score: 0.2640 - top_2_accuracy: 0.6761 - val_loss: 1.2515 - val_accuracy: 0.5120 - val_f1_score: 0.3114 - val_top_2_accuracy: 0.7724 - lr: 1.0000e-05\n",
            "Epoch 108/500\n",
            "312/312 [==============================] - 261s 838ms/step - loss: 6.0817 - accuracy: 0.3884 - f1_score: 0.2852 - top_2_accuracy: 0.6971 - val_loss: 1.1532 - val_accuracy: 0.6398 - val_f1_score: 0.3325 - val_top_2_accuracy: 0.7994 - lr: 1.0000e-05\n",
            "Epoch 109/500\n",
            "312/312 [==============================] - 261s 838ms/step - loss: 6.3343 - accuracy: 0.3728 - f1_score: 0.2644 - top_2_accuracy: 0.6789 - val_loss: 1.3156 - val_accuracy: 0.4781 - val_f1_score: 0.2864 - val_top_2_accuracy: 0.7168 - lr: 1.0000e-05\n",
            "Epoch 110/500\n",
            "312/312 [==============================] - 262s 839ms/step - loss: 6.2462 - accuracy: 0.3558 - f1_score: 0.2650 - top_2_accuracy: 0.6855 - val_loss: 1.2428 - val_accuracy: 0.4043 - val_f1_score: 0.2937 - val_top_2_accuracy: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 111/500\n",
            "312/312 [==============================] - 263s 841ms/step - loss: 6.6029 - accuracy: 0.3389 - f1_score: 0.2691 - top_2_accuracy: 0.6763 - val_loss: 1.2722 - val_accuracy: 0.4977 - val_f1_score: 0.3077 - val_top_2_accuracy: 0.7575 - lr: 1.0000e-05\n",
            "Epoch 112/500\n",
            "312/312 [==============================] - 261s 837ms/step - loss: 5.9134 - accuracy: 0.3758 - f1_score: 0.2749 - top_2_accuracy: 0.7021 - val_loss: 1.0555 - val_accuracy: 0.6745 - val_f1_score: 0.3696 - val_top_2_accuracy: 0.8457 - lr: 1.0000e-05\n",
            "Epoch 113/500\n",
            "312/312 [==============================] - 266s 851ms/step - loss: 6.1296 - accuracy: 0.3900 - f1_score: 0.2847 - top_2_accuracy: 0.6869 - val_loss: 1.1203 - val_accuracy: 0.6140 - val_f1_score: 0.3605 - val_top_2_accuracy: 0.8069 - lr: 1.0000e-05\n",
            "Epoch 114/500\n",
            "312/312 [==============================] - 261s 837ms/step - loss: 6.4160 - accuracy: 0.3620 - f1_score: 0.2746 - top_2_accuracy: 0.6949 - val_loss: 1.1270 - val_accuracy: 0.5763 - val_f1_score: 0.3351 - val_top_2_accuracy: 0.8028 - lr: 1.0000e-05\n",
            "Epoch 115/500\n",
            "312/312 [==============================] - 262s 839ms/step - loss: 6.5165 - accuracy: 0.3534 - f1_score: 0.2627 - top_2_accuracy: 0.6791 - val_loss: 1.2073 - val_accuracy: 0.5108 - val_f1_score: 0.3159 - val_top_2_accuracy: 0.7835 - lr: 1.0000e-05\n",
            "Epoch 116/500\n",
            "312/312 [==============================] - 262s 841ms/step - loss: 6.1476 - accuracy: 0.3590 - f1_score: 0.2638 - top_2_accuracy: 0.7029 - val_loss: 1.2014 - val_accuracy: 0.4637 - val_f1_score: 0.3029 - val_top_2_accuracy: 0.7722 - lr: 1.0000e-05\n",
            "Epoch 117/500\n",
            "312/312 [==============================] - 266s 853ms/step - loss: 6.4880 - accuracy: 0.3518 - f1_score: 0.2835 - top_2_accuracy: 0.6797 - val_loss: 1.2345 - val_accuracy: 0.4099 - val_f1_score: 0.2800 - val_top_2_accuracy: 0.7597 - lr: 1.0000e-05\n",
            "Epoch 118/500\n",
            "312/312 [==============================] - 290s 930ms/step - loss: 5.9860 - accuracy: 0.3542 - f1_score: 0.2685 - top_2_accuracy: 0.6803 - val_loss: 1.4099 - val_accuracy: 0.4361 - val_f1_score: 0.2591 - val_top_2_accuracy: 0.6979 - lr: 1.0000e-05\n",
            "Epoch 119/500\n",
            "312/312 [==============================] - ETA: 0s - loss: 6.1468 - accuracy: 0.4197 - f1_score: 0.2893 - top_2_accuracy: 0.7167"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7vnddzcvjMR",
        "colab_type": "text"
      },
      "source": [
        "# GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udY7sohtETlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboard.plugins.hparams import api as hp\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eeYLWP4uBkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_model(hparams):\n",
        "  train, train_count, val, val_count = create_datasets(train_df, val_df, hparams[HP_BATCH_NUM])\n",
        "  \n",
        "  base_model = InceptionV3(weights='imagenet', \n",
        "                           include_top=False, \n",
        "                           input_shape=get_input_shape())\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "\n",
        "  # let's add a fully-connected layer\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(4086, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  x = Dense(4086, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  predictions = Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  # first: train only the top layers (which were randomly initialized)\n",
        "  # i.e. freeze all convolutional InceptionV3 layers\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    \n",
        "  # compile the model (should be done *after* setting layers to non-trainable)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR]), loss='categorical_crossentropy')\n",
        "\n",
        "  # train the model on the new data for a few epochs\n",
        "  # model.fit(train, steps_per_epoch=train_count // hparams[HP_BATCH_NUM], epochs=3,\n",
        "  #         class_weight=weights)\n",
        "\n",
        "  # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "  for layer in model.layers[:hparams[HP_FREEZE_LAYERS]]:\n",
        "    layer.trainable = False\n",
        "  for layer in model.layers[hparams[HP_FREEZE_LAYERS]:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "  # we need to recompile the model for these modifications to take effect\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR]), \n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=get_metrics())\n",
        "  \n",
        "  model.fit(train, steps_per_epoch=train_count // hparams[HP_BATCH_NUM], \n",
        "            epochs=15) # Run with 1 epoch to speed things up for demo purposes\n",
        "  metrics = model.evaluate(val, steps=val_count // hparams[HP_BATCH_NUM])\n",
        "  return {k : metrics[i] for i, k in enumerate(model.metrics_names)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGu6t40vq_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./logs/ \n",
        "\n",
        "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "HP_LR = hp.HParam('lr', hp.Discrete([1e-2]))\n",
        "# HP_LR = hp.HParam('lr', hp.Discrete([1e-2, 1e-3, 1e-4]))\n",
        "HP_BATCH_NUM = hp.HParam('batch_num', hp.Discrete([8]))\n",
        "# WITHOUT_MOMENTUM = 0.0\n",
        "# HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([WITHOUT_MOMENTUM, 0.5, 0.7, 0.9]))\n",
        "# HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([None]))\n",
        "HP_DROP = hp.HParam('drop', hp.Discrete([0.5]))\n",
        "HP_FREEZE_LAYERS = hp.HParam('freeze_layers', hp.Discrete([172]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "METRIC_F1 = 'f1_score'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_LR, HP_BATCH_NUM, HP_DROP, HP_FREEZE_LAYERS],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='accuracy'), \n",
        "             hp.Metric(METRIC_F1, display_name='f1_score')],\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv03N4-xwl7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "\n",
        "    # if hparams[HP_OPTIMIZER] == 'sgd':\n",
        "    #   hparams[HP_OPTIMIZER] = tf.keras.optimizers.SGD(\n",
        "    #     learning_rate=hparams[HP_LR],\n",
        "    #     momentum=hparams[HP_MOMENTUM]) \n",
        "    # else:\n",
        "    #   hparams[HP_OPTIMIZER] = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR])\n",
        "\n",
        "    metrics = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, metrics[METRIC_ACCURACY], step=1)\n",
        "    f1_scores_each_class = metrics[METRIC_F1]\n",
        "    f1_score_avg = sum(f1_scores_each_class) / len(f1_scores_each_class)\n",
        "    tf.summary.scalar(METRIC_F1,f1_score_avg, step=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgTprnhMw0ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for batch_num in HP_BATCH_NUM.domain.values:\n",
        "  for lr in HP_LR.domain.values:\n",
        "    for drop in HP_DROP.domain.values:\n",
        "      for fl in HP_FREEZE_LAYERS.domain.values:\n",
        "        hparams = {\n",
        "            HP_LR: lr,\n",
        "            HP_BATCH_NUM: batch_num,\n",
        "            HP_DROP: drop,\n",
        "            HP_FREEZE_LAYERS: fl\n",
        "        }\n",
        "        run_name = \"run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run('logs/hparam_tuning/' + run_name, hparams)\n",
        "        session_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxJ_AQGSTrBV",
        "colab_type": "text"
      },
      "source": [
        "# Other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWGuIDPbbt88",
        "colab_type": "text"
      },
      "source": [
        "Correctness of loss [CS231](http://cs231n.github.io/neural-networks-3/#sanitycheck) <font color='green'>OK</font>\n",
        "\n",
        "Loss should be equal -ln(1/len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoeLkz8oW_uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "-np.log(1/len(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gk2mMmthCR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df['level'].value_counts().plot(kind='bar', figsize=(5,2), title='Level distribution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUXrNAEJhH8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrrVpkAiS-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, label_batch = next(iter(val_ds))\n",
        "model.evaluate(image_batch, label_batch, BATCH_SIZE)\n",
        "predicted_probabilities = model.predict(image_batch)\n",
        "print('Predicted probabilities:', predicted_probabilities)\n",
        "predicted_labels = np.argmax(predicted_probabilities,axis=1)\n",
        "print('Labels:', np.argmax(label_batch,axis=1))\n",
        "print('Predicted labels:', predicted_labels)\n",
        "show_batch(image_batch.numpy(), label_batch.numpy(), number_to_show=20,predicted_labels=predicted_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp2jNhuFicJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir 'drive/My Drive/diabetic-retinopathy-thesis/Logs/20200507-001110'\n",
        "# %tensorboard --logdir 'logs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jcQoPAuigOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model = tf.keras.models.load_model(\n",
        "    os.path.join(PROJECT_DIR, 'models', 'model.17-0.76.hdf5'),\n",
        "    {'top_2_accuracy' : top_2_accuracy}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}