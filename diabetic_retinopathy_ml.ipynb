{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN5znD6g_vFV",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tokarchyn/diabetic-retinopathy-thesis/blob/master/diabetic-retinopathy-ml.ipynb\" target=\"_parent\">     \n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-XAUTpUtth",
        "colab_type": "text"
      },
      "source": [
        "# Imports and methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQKx-pJN_BNx",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import argparse\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "from IPython.display import display, clear_output\n",
        "import glob\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# %precision % .5f\n",
        "np.set_printoptions(suppress=True, precision=5)\n",
        "\n",
        "# Define constants\n",
        "CLASS_NAMES = np.array(\n",
        "    ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR'])\n",
        "CLASS_INDEXES = [0, 1, 2, 3, 4]\n",
        "\n",
        "\n",
        "def fetch_data_from_gdrive(project_dir, remote_project_dir, zip_name):\n",
        "    target_zip = os.path.join(project_dir, zip_name)\n",
        "    # !mkdir - p \"{project_dir}\"\n",
        "    # !cp \"{remote_project_dir}/{zip_name}\" \"{target_zip}\"\n",
        "    # !unzip - q \"{target_zip}\"\n",
        "    # !rm \"{target_zip}\"\n",
        "\n",
        "\n",
        "def init_env():\n",
        "    args = {}\n",
        "    if IN_COLAB:\n",
        "        args['remote_project_dir'] = 'drive/My Drive/diabetic-retinopathy-thesis'\n",
        "        args['project_dir'] = '/content'\n",
        "        args['image_dir'] = os.path.join(\n",
        "            args['project_dir'], 'train_processed')\n",
        "        args['dataframe_path'] = os.path.join(\n",
        "            args['remote_project_dir'], 'trainLabels.csv')\n",
        "        args['experiments_dir'] = os.path.join(\n",
        "            args['remote_project_dir'], 'experiments')\n",
        "        args['quality_dataset_path'] = 'image_quality.csv'\n",
        "\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        fetch_data_from_gdrive(args['project_dir'],\n",
        "                               args['remote_project_dir'],\n",
        "                               'train_processed.zip')\n",
        "    else:\n",
        "        old_argv = sys.argv\n",
        "        if sys.argv[-1].endswith('json'):\n",
        "            sys.argv = ['']\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--image_dir', type=str, default='train_processed')\n",
        "        parser.add_argument('--dataframe_path', type=str,\n",
        "                            default='trainLabels.csv')\n",
        "        parser.add_argument('--quality_dataset_path',\n",
        "                            type=str, default='image_quality.csv')\n",
        "        parser.add_argument('--experiments_dir', type=str,\n",
        "                            default='experiments')\n",
        "        parsed_args = parser.parse_args()\n",
        "        args = vars(parsed_args)\n",
        "        sys.argv = old_argv\n",
        "\n",
        "    args['img_size'] = 512\n",
        "    args['batch_size'] = 32\n",
        "    print('Arguments:', json.dumps(args))\n",
        "    return args\n",
        "\n",
        "# Methods to process dataframe\n",
        "\n",
        "\n",
        "def load_df(dataframe_path, base_image_dir):\n",
        "    df = pd.read_csv(dataframe_path)\n",
        "    df['image_path'] = df['image'].astype(str).apply(\n",
        "        lambda x: os.path.join(base_image_dir, x + '.jpeg'))\n",
        "    df = df.drop(columns=['image'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_unexist(df, base_image_dir):\n",
        "    all_images = glob.glob(base_image_dir + \"/*\")\n",
        "    while len(all_images) == 0:\n",
        "        all_images = glob.glob(base_image_dir + \"/*\")\n",
        "    print('Found', len(all_images), 'images')\n",
        "    df['exists'] = df['image_path'].map(lambda p: p in all_images)\n",
        "    df = df[df['exists']].drop(columns=['exists'])\n",
        "    print('Number of existed images is', len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "def train_val_split(df):\n",
        "    train_img, val_img = train_test_split(df['image_path'],\n",
        "                                          test_size=0.20,\n",
        "                                          random_state=2020,\n",
        "                                          stratify=df['level'].to_frame())\n",
        "    train_df = df[df['image_path'].isin(train_img)]\n",
        "    val_df = df[df['image_path'].isin(val_img)]\n",
        "    print('Train dataframe size:',\n",
        "          train_df.shape[0], 'Validation dataframe size:', val_df.shape[0])\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def calc_weights(df):\n",
        "    level_counts = df['level'].value_counts().sort_index()\n",
        "    weights = {cls: len(df) / count for cls, count in enumerate(level_counts)}\n",
        "    if max(weights.values()) - min(weights.values()) < 0.1:\n",
        "        print('Reset weights because they all are the same:', max(weights.values()))\n",
        "        weights = None\n",
        "    else:\n",
        "        print('Weights for each level:\\n', weights)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def balance(df, counts):\n",
        "    new_df = df.iloc[0:0]  # copy only structure\n",
        "    for level in df['level'].unique():\n",
        "        df_level = df[df['level'] == level]\n",
        "        count = len(df_level)\n",
        "        new_count = counts[level] if level in counts else count\n",
        "        if count > new_count:\n",
        "            new_df = new_df.append(\n",
        "                df_level.drop(df_level.sample(count - new_count).index),\n",
        "                ignore_index=True)\n",
        "        elif count < new_count:\n",
        "            new_df = new_df.append(df_level, ignore_index=True)\n",
        "            new_df = new_df.append(df_level.sample(\n",
        "                new_count - count, replace=True), ignore_index=True)\n",
        "\n",
        "    print('New counts of dataset\\'s categories: ', json.dumps(\n",
        "        new_df['level'].value_counts().to_dict()))\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def balance_with_mode(df, mode='max'):\n",
        "    counts = df['level'].value_counts()\n",
        "    new_count = 0\n",
        "    if mode == 'max':\n",
        "        new_count = counts.max()\n",
        "    elif mode == 'min':\n",
        "        new_count = counts.min()\n",
        "    new_counts_dict = {level: new_count for level in df['level'].unique()}\n",
        "    return balance(df, counts=new_counts_dict)\n",
        "\n",
        "\n",
        "def shuffle(df):\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def exclude_by_quality(df, quality_dataset_path):\n",
        "    quality_dict = pd.read_csv(quality_dataset_path, index_col='image_name')[\n",
        "        'quality'].to_dict()\n",
        "    select = df['image_path'].apply(lambda p: Path(\n",
        "        p).stem not in quality_dict or quality_dict[Path(p).stem] == 0)\n",
        "    print('Images to exclude:', len(select[select == False]))\n",
        "    df = df.loc[select]\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_data(dataframe_path, base_image_dir, quality_dataset_path):\n",
        "    if not os.path.exists(base_image_dir):\n",
        "        raise NameError('Base image path doesnt exist', base_image_dir)\n",
        "    df = load_df(dataframe_path, base_image_dir)\n",
        "    df = remove_unexist(df, base_image_dir)\n",
        "    df = shuffle(df)\n",
        "    # df = shrink_dataset(df, 1000)\n",
        "\n",
        "    train_df, val_df = train_val_split(df)\n",
        "    train_df = exclude_by_quality(train_df, quality_dataset_path)\n",
        "    # train_df = balance_with_mode(train_df, mode='max') # take the same number of samples as majority category has\n",
        "    # take some samples from each category\n",
        "    train_df = balance(train_df, counts={\n",
        "                       0: 6000, 1: 6000, 2: 6000, 3: 6000, 4: 6000})\n",
        "    # train_df = balance_with_mode(train_df, mode='min') # take the same number of samples as minority category has\n",
        "    train_df = shuffle(train_df)\n",
        "    weights = calc_weights(train_df)\n",
        "\n",
        "    return train_df, val_df, weights\n",
        "\n",
        "# Augmentation functions\n",
        "\n",
        "\n",
        "def rotate(x):\n",
        "    # Rotate 0, 90, 180, 270 degrees\n",
        "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "\n",
        "\n",
        "def flip(x):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_flip_up_down(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def color(x):\n",
        "    x = tf.image.random_hue(x, 0.04)\n",
        "    x = tf.image.random_saturation(x, 0.9, 1.1)\n",
        "    x = tf.image.random_brightness(x, 0.04)\n",
        "    x = tf.image.random_contrast(x, 0.9, 1.1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def zoom(x, img_size):\n",
        "    # Generate 20 crop settings, ranging from a 1% to 10% crop.\n",
        "    scales = list(np.arange(0.9, 1, 0.01))\n",
        "    boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - (0.5 * scale)\n",
        "        x2 = y2 = 0.5 + (0.5 * scale)\n",
        "        boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "    def random_crop(img):\n",
        "        # Create different crops for an image\n",
        "        crops = tf.image.crop_and_resize([img], boxes=boxes,\n",
        "                                         box_indices=np.zeros(len(scales)),\n",
        "                                         crop_size=(img_size, img_size))\n",
        "        # Return a random crop\n",
        "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "\n",
        "    choice = tf.random.uniform(\n",
        "        shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "\n",
        "    # Only apply cropping 50% of the time\n",
        "    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n",
        "\n",
        "\n",
        "def augment(dataset, img_size, aug_probability=1):\n",
        "    def zoom_local(x): return zoom(x, img_size)\n",
        "    augmentations = [flip, rotate, color, zoom_local]\n",
        "\n",
        "    def augment_map(img, level, aug_fun):\n",
        "        return (aug_fun(img), level)\n",
        "        # return (tf.cond(tf.math.argmax(level, axis = 0) == 0, lambda: img, lambda: aug_fun(img)), level)\n",
        "        # choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "        # return (tf.cond(choice < aug_probability, lambda: img, lambda: aug_fun(img)),\n",
        "        #         level)\n",
        "\n",
        "    # Add the augmentations to the dataset\n",
        "    for f in augmentations:\n",
        "        dataset = dataset.map(lambda img, level: augment_map(\n",
        "            img, level, f), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    # Make sure that the values are still in [0, 1]\n",
        "    dataset = dataset.map(lambda img, level: (\n",
        "        tf.clip_by_value(img, 0, 1), level), num_parallel_calls=AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Create Tensorflow's dataset\n",
        "\n",
        "\n",
        "def get_input_shape(img_size):\n",
        "    return (img_size, img_size, 3)\n",
        "\n",
        "\n",
        "def decode_img(img, img_size):\n",
        "    # convert the compressed string to a 3D uint8 tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # resize the image to the desired size.\n",
        "    return tf.image.resize(img, [img_size, img_size])\n",
        "\n",
        "\n",
        "def get_label(level):\n",
        "    return tf.cast(level == CLASS_INDEXES, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def process_path(file_path, level, img_size):\n",
        "    label = get_label(level)\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = decode_img(img, img_size)\n",
        "    return img, label\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle_buffer_size=200):\n",
        "    ds = ds.map(lambda img, level: (tf.image.per_image_standardization(img), level),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "def dataset_from_tensor_slices(df):\n",
        "    return tf.data.Dataset.from_tensor_slices((\n",
        "        df['image_path'].to_numpy(copy=True),\n",
        "        df['level'].to_numpy(copy=True)))\n",
        "\n",
        "\n",
        "def create_datasets(train_df, val_df, img_size, batch_size):\n",
        "    train_ds = dataset_from_tensor_slices(train_df)\n",
        "    val_ds = dataset_from_tensor_slices(val_df)\n",
        "\n",
        "    def process_path_local(file_path, level): return process_path(\n",
        "        file_path, level, img_size)\n",
        "\n",
        "    train_ds = train_ds.map(process_path_local, num_parallel_calls=AUTOTUNE)\n",
        "    train_ds = augment(train_ds, img_size)\n",
        "    train_ds = prepare(train_ds)\n",
        "    train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "    val_ds = val_ds.map(process_path_local, num_parallel_calls=AUTOTUNE)\n",
        "    val_ds = prepare(val_ds)\n",
        "    val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "    return train_ds, len(train_df), val_ds, len(val_df)\n",
        "\n",
        "# Visualisation\n",
        "\n",
        "\n",
        "def show_batch(image_batch, label_batch, number_to_show=4, predicted_labels=None):\n",
        "    row_count = math.ceil(number_to_show / 4)\n",
        "    fig, m_axs = plt.subplots(row_count, 4, figsize=(16, row_count * 4))\n",
        "    for i, (c_x, c_y, c_ax) in enumerate(zip(image_batch, label_batch, m_axs.flatten())):\n",
        "        c_ax.imshow(c_x)\n",
        "        real_level = CLASS_NAMES[c_y == 1][0]\n",
        "        pred_level = ''\n",
        "        title = 'Real level: ' + real_level\n",
        "        if predicted_labels is not None:\n",
        "            pred_level = CLASS_NAMES[predicted_labels[i]]\n",
        "            title = title + '\\nPredicted one: ' + pred_level\n",
        "        c_ax.set_title(title, color='g' if pred_level ==\n",
        "                       '' or real_level == pred_level else 'r')\n",
        "        c_ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_metric(metrics, metric_name, save_dest=None):\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plt.plot(metrics[metric_name])\n",
        "    plt.plot(metrics['val_' + metric_name])\n",
        "    plt.title(metric_name)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "    if save_dest:\n",
        "        plt.savefig(os.path.join(save_dest, metric_name + '.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_f1(metrics, save_dest=None):\n",
        "    fig, m_axs = plt.subplots(2, 1, figsize=(12, 16))\n",
        "\n",
        "    m_axs[0].plot(metrics['f1_score'])\n",
        "    m_axs[0].set_ylabel('f1_score')\n",
        "    m_axs[0].set_xlabel('epoch')\n",
        "    m_axs[0].set_title('Training')\n",
        "    m_axs[0].legend(CLASS_NAMES, loc='upper left')\n",
        "\n",
        "    m_axs[1].plot(metrics['val_f1_score'])\n",
        "    m_axs[1].set_ylabel('val_f1_score')\n",
        "    m_axs[1].set_xlabel('epoch')\n",
        "    m_axs[1].set_title('Validation')\n",
        "    m_axs[1].legend(CLASS_NAMES, loc='upper left')\n",
        "\n",
        "    if save_dest:\n",
        "        fig.savefig(os.path.join(save_dest, 'f1_score.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "    plot_metric({\n",
        "        \"f1_score_average\": np.array(metrics['f1_score']).mean(axis=1),\n",
        "        \"val_f1_score_average\": np.array(metrics['val_f1_score']).mean(axis=1)},\n",
        "        'f1_score_average',\n",
        "        save_dest)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_lables, pred_labels, target_names, save_dest=None):\n",
        "    cm = confusion_matrix(true_lables, pred_labels)\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    thresh = cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    if save_dest:\n",
        "        fig.savefig(os.path.join(save_dest, 'confusion_matrix.png'))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "# Callbacks and metrics\n",
        "# Helps to persist all metrics and weights in situation when you interupt training\n",
        "\n",
        "\n",
        "class TrainingHistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, metrics, save_weights=True, metrics_plot_dir=None):\n",
        "        self.metrics = {}\n",
        "        self.weights = []\n",
        "        self.save_weights = save_weights\n",
        "        self.metrics_plot_dir = metrics_plot_dir\n",
        "        for m in metrics:\n",
        "            self.metrics[m] = []\n",
        "            self.metrics['val_' + m] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            for key in self.metrics.keys():\n",
        "                self.metrics[key].append(logs.get(key))\n",
        "\n",
        "        if self.metrics_plot_dir:\n",
        "            for key in self.metrics.keys():\n",
        "                if not key.startswith('val_'):\n",
        "                    if key == 'f1_score':\n",
        "                        plot_f1(self.metrics, self.metrics_plot_dir)\n",
        "                    else:\n",
        "                        plot_metric(self.metrics, key, self.metrics_plot_dir)\n",
        "\n",
        "        if self.save_weights:\n",
        "            self.weights.append([])\n",
        "            for l in model.layers:\n",
        "                self.weights[len(self.weights)-1].append(l.get_weights())\n",
        "\n",
        "\n",
        "def get_callbacks(save_best_models=True, best_models_dir=None,\n",
        "                  early_stopping=True,\n",
        "                  reduce_lr_on_plateau=True,\n",
        "                  training_history=True, metrics_plot_dir=None):\n",
        "    callbacks = []\n",
        "    if save_best_models:\n",
        "        Path(best_models_dir).mkdir(parents=True, exist_ok=True)\n",
        "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(\n",
        "                best_models_dir, 'e_{epoch:02d}-acc_{val_accuracy:.2f}-f1_{val_f1_score}.hdf5'),\n",
        "            monitor='val_accuracy',\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='auto'))\n",
        "    if early_stopping:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=20,\n",
        "            restore_best_weights=True))\n",
        "    if reduce_lr_on_plateau:\n",
        "        callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.8,\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            mode='auto',\n",
        "            epsilon=0.0001,\n",
        "            cooldown=5,\n",
        "            min_lr=0.00001))\n",
        "    if training_history:\n",
        "        Path(metrics_plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "        callbacks.append(TrainingHistoryCallback(\n",
        "            ['loss', 'accuracy', 'f1_score'],\n",
        "            metrics_plot_dir=metrics_plot_dir))\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "\n",
        "def top_2_accuracy(in_gt, in_pred):\n",
        "    return tf.keras.metrics.top_k_categorical_accuracy(in_gt, in_pred, k=2)\n",
        "\n",
        "\n",
        "class FBetaScore(tf.keras.metrics.Metric):\n",
        "    \"\"\"Computes F-Beta score.\n",
        "    It is the weighted harmonic mean of precision\n",
        "    and recall. Output range is [0, 1]. Works for\n",
        "    both multi-class and multi-label classification.\n",
        "    F-Beta = (1 + beta^2) * (prec * recall) / ((beta^2 * prec) + recall)\n",
        "    Args:\n",
        "        num_classes: Number of unique classes in the dataset.\n",
        "        average: Type of averaging to be performed on data.\n",
        "            Acceptable values are `None`, `micro`, `macro` and\n",
        "            `weighted`. Default value is None.\n",
        "        beta: Determines the weight of precision and recall\n",
        "            in harmonic mean. Determines the weight given to the\n",
        "            precision and recall. Default value is 1.\n",
        "        threshold: Elements of `y_pred` greater than threshold are\n",
        "            converted to be 1, and the rest 0. If threshold is\n",
        "            None, the argmax is converted to 1, and the rest 0.\n",
        "    Returns:\n",
        "        F-Beta Score: float\n",
        "    Raises:\n",
        "        ValueError: If the `average` has values other than\n",
        "        [None, micro, macro, weighted].\n",
        "        ValueError: If the `beta` value is less than or equal\n",
        "        to 0.\n",
        "    `average` parameter behavior:\n",
        "        None: Scores for each class are returned\n",
        "        micro: True positivies, false positives and\n",
        "            false negatives are computed globally.\n",
        "        macro: True positivies, false positives and\n",
        "            false negatives are computed for each class\n",
        "            and their unweighted mean is returned.\n",
        "        weighted: Metrics are computed for each class\n",
        "            and returns the mean weighted by the\n",
        "            number of true instances in each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        average=None,\n",
        "        beta=1.0,\n",
        "        threshold=None,\n",
        "        name=\"fbeta_score\",\n",
        "        dtype=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(name=name, dtype=dtype)\n",
        "\n",
        "        if average not in (None, \"micro\", \"macro\", \"weighted\"):\n",
        "            raise ValueError(\n",
        "                \"Unknown average type. Acceptable values \"\n",
        "                \"are: [None, micro, macro, weighted]\"\n",
        "            )\n",
        "\n",
        "        if not isinstance(beta, float):\n",
        "            raise TypeError(\"The value of beta should be a python float\")\n",
        "\n",
        "        if beta <= 0.0:\n",
        "            raise ValueError(\"beta value should be greater than zero\")\n",
        "\n",
        "        if threshold is not None:\n",
        "            if not isinstance(threshold, float):\n",
        "                raise TypeError(\n",
        "                    \"The value of threshold should be a python float\")\n",
        "            if threshold > 1.0 or threshold <= 0.0:\n",
        "                raise ValueError(\"threshold should be between 0 and 1\")\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.average = average\n",
        "        self.beta = beta\n",
        "        self.threshold = threshold\n",
        "        self.axis = None\n",
        "        self.init_shape = []\n",
        "\n",
        "        if self.average != \"micro\":\n",
        "            self.axis = 0\n",
        "            self.init_shape = [self.num_classes]\n",
        "\n",
        "        def _zero_wt_init(name):\n",
        "            return self.add_weight(\n",
        "                name, shape=self.init_shape, initializer=\"zeros\", dtype=self.dtype\n",
        "            )\n",
        "\n",
        "        self.true_positives = _zero_wt_init(\"true_positives\")\n",
        "        self.false_positives = _zero_wt_init(\"false_positives\")\n",
        "        self.false_negatives = _zero_wt_init(\"false_negatives\")\n",
        "        self.weights_intermediate = _zero_wt_init(\"weights_intermediate\")\n",
        "\n",
        "    # TODO: Add sample_weight support, currently it is\n",
        "    # ignored during calculations.\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        if self.threshold is None:\n",
        "            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "            # make sure [0, 0, 0] doesn't become [1, 1, 1]\n",
        "            # Use abs(x) > eps, instead of x != 0 to check for zero\n",
        "            y_pred = tf.logical_and(\n",
        "                y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
        "        else:\n",
        "            y_pred = y_pred > self.threshold\n",
        "\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.cast(y_pred, tf.int32)\n",
        "\n",
        "        def _count_non_zero(val):\n",
        "            non_zeros = tf.math.count_nonzero(val, axis=self.axis)\n",
        "            return tf.cast(non_zeros, self.dtype)\n",
        "\n",
        "        self.true_positives.assign_add(_count_non_zero(y_pred * y_true))\n",
        "        self.false_positives.assign_add(_count_non_zero(y_pred * (y_true - 1)))\n",
        "        self.false_negatives.assign_add(_count_non_zero((y_pred - 1) * y_true))\n",
        "        self.weights_intermediate.assign_add(_count_non_zero(y_true))\n",
        "\n",
        "    def result(self):\n",
        "        precision = tf.math.divide_no_nan(\n",
        "            self.true_positives, self.true_positives + self.false_positives\n",
        "        )\n",
        "        recall = tf.math.divide_no_nan(\n",
        "            self.true_positives, self.true_positives + self.false_negatives\n",
        "        )\n",
        "\n",
        "        mul_value = precision * recall\n",
        "        add_value = (tf.math.square(self.beta) * precision) + recall\n",
        "        mean = tf.math.divide_no_nan(mul_value, add_value)\n",
        "        f1_score = mean * (1 + tf.math.square(self.beta))\n",
        "\n",
        "        if self.average == \"weighted\":\n",
        "            weights = tf.math.divide_no_nan(\n",
        "                self.weights_intermediate, tf.reduce_sum(\n",
        "                    self.weights_intermediate)\n",
        "            )\n",
        "            f1_score = tf.reduce_sum(f1_score * weights)\n",
        "\n",
        "        elif self.average is not None:  # [micro, macro]\n",
        "            f1_score = tf.reduce_mean(f1_score)\n",
        "\n",
        "        return f1_score\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Returns the serializable config of the metric.\"\"\"\n",
        "\n",
        "        config = {\n",
        "            \"num_classes\": self.num_classes,\n",
        "            \"average\": self.average,\n",
        "            \"beta\": self.beta,\n",
        "        }\n",
        "\n",
        "        if self.threshold is not None:\n",
        "            config[\"threshold\"] = self.threshold\n",
        "\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.false_positives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.false_negatives.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "        self.weights_intermediate.assign(tf.zeros(self.init_shape, self.dtype))\n",
        "\n",
        "\n",
        "class F1Score(FBetaScore):\n",
        "    \"\"\"Computes F-1 Score.\n",
        "    It is the harmonic mean of precision and recall.\n",
        "    Output range is [0, 1]. Works for both multi-class\n",
        "    and multi-label classification.\n",
        "    F-1 = 2 * (precision * recall) / (precision + recall)\n",
        "    Args:\n",
        "        num_classes: Number of unique classes in the dataset.\n",
        "        average: Type of averaging to be performed on data.\n",
        "            Acceptable values are `None`, `micro`, `macro`\n",
        "            and `weighted`. Default value is None.\n",
        "        threshold: Elements of `y_pred` above threshold are\n",
        "            considered to be 1, and the rest 0. If threshold is\n",
        "            None, the argmax is converted to 1, and the rest 0.\n",
        "    Returns:\n",
        "        F-1 Score: float\n",
        "    Raises:\n",
        "        ValueError: If the `average` has values other than\n",
        "        [None, micro, macro, weighted].\n",
        "    `average` parameter behavior:\n",
        "        None: Scores for each class are returned\n",
        "        micro: True positivies, false positives and\n",
        "            false negatives are computed globally.\n",
        "        macro: True positivies, false positives and\n",
        "            false negatives are computed for each class\n",
        "            and their unweighted mean is returned.\n",
        "        weighted: Metrics are computed for each class\n",
        "            and returns the mean weighted by the\n",
        "            number of true instances in each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        average=None,\n",
        "        threshold=None,\n",
        "        name=\"f1_score\",\n",
        "        dtype=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(num_classes, average, 1.0, threshold, name=name, dtype=dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        del base_config[\"beta\"]\n",
        "        return base_config\n",
        "\n",
        "\n",
        "def get_metrics():\n",
        "    return ['accuracy', F1Score(len(CLASS_INDEXES)), top_2_accuracy]\n",
        "\n",
        "# Models\n",
        "\n",
        "\n",
        "def get_alex_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(96, (11, 11), strides=4, activation=\"relu\",  padding=\"same\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (11, 11), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(384, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(384, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), strides=1,\n",
        "                     padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), strides=1,\n",
        "                     padding=\"same\",  activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3),  padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2), strides=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"categorical_crossentropy\", metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_custom_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                     activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2048, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation='softmax'))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy', metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_vgg_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "                  metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_inception_v3(train_ds, train_steps, weights, freeze_layers_number, input_shape):\n",
        "    # create the base pre-trained model\n",
        "    base_model = InceptionV3(weights='imagenet',\n",
        "                             include_top=False,\n",
        "                             input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # first: train only the top layers (which were randomly initialized)\n",
        "    # i.e. freeze all convolutional InceptionV3 layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # compile the model (should be done *after* setting layers to non-trainable)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.001), loss='categorical_crossentropy')\n",
        "\n",
        "    # train the model on the new data for a few epochs\n",
        "    model.fit(train_ds, steps_per_epoch=train_steps,\n",
        "              epochs=3, class_weight=weights)\n",
        "\n",
        "    for layer in model.layers[:freeze_layers_number]:\n",
        "        layer.trainable = False\n",
        "    for layer in model.layers[freeze_layers_number:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # we need to recompile the model for these modifications to take effect\n",
        "    # we use SGD with a low learning rate\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_all_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (7, 7), strides=1, activation=\"relu\",  padding=\"valid\",\n",
        "                     input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (5, 5), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3, 3), strides=2, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3), strides=2, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3, 3), strides=2,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, (1, 1), strides=1,\n",
        "                     activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(5, (1, 1), strides=1, activation=\"relu\",  padding=\"valid\"))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(len(CLASS_NAMES), activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"categorical_crossentropy\", metrics=get_metrics())\n",
        "    return model\n",
        "\n",
        "# Train and validation\n",
        "\n",
        "\n",
        "def train(model, train_ds, train_steps, val_ds, val_steps,\n",
        "          experiment_dir, epochs, weights=None):\n",
        "    print('Start training.')\n",
        "    history = model.fit(train_ds, steps_per_epoch=train_steps,\n",
        "                        validation_data=val_ds, validation_steps=val_steps,\n",
        "                        class_weight=weights,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=get_callbacks(\n",
        "                            save_best_models=False,\n",
        "                            best_models_dir=os.path.join(\n",
        "                                experiment_dir, 'models'),\n",
        "                            early_stopping=False,\n",
        "                            reduce_lr_on_plateau=True,\n",
        "                            training_history=True,\n",
        "                            metrics_plot_dir=experiment_dir)\n",
        "                        )\n",
        "    print('Training finished.')\n",
        "    return history\n",
        "\n",
        "\n",
        "def create_confusion_matrix(model, dataset, steps, target_names, save_dest=None):\n",
        "    print('Creating confusion matrix.')\n",
        "    it = iter(dataset)\n",
        "    true_labels_glob = []\n",
        "    pred_labels_glob = []\n",
        "\n",
        "    for i in range(0, steps):\n",
        "        image_batch, true_labels = next(it)\n",
        "        true_labels_glob.extend(np.argmax(true_labels, axis=1))\n",
        "        pred = model.predict(image_batch)\n",
        "        pred_labels_glob.extend(np.argmax(pred, axis=1))\n",
        "\n",
        "    plot_confusion_matrix(\n",
        "        true_labels_glob, pred_labels_glob, target_names, save_dest)\n",
        "    print('Confusion matrix was saved to', save_dest)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUiVTTjCLOtv",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Kv5snxyRlTh",
        "outputId": "f02fe249-e017-4041-aff1-b15c0452c3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Get args\n",
        "args = init_env()\n",
        "\n",
        "# Create input objects\n",
        "train_df, val_df, weights = prepare_data(\n",
        "    args['dataframe_path'], args['image_dir'], args['quality_dataset_path'])\n",
        "train_ds, train_count, val_ds, val_count = create_datasets(\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    img_size=args['img_size'],\n",
        "    batch_size=args['batch_size'])\n",
        "train_steps = 5000 // args['batch_size'] #train_count // args['batch_size']\n",
        "\n",
        "# Create model\n",
        "input_shape = get_input_shape(args['img_size'])\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    print('There is no model defined')\n",
        "# model = get_model(input_shape)\n",
        "# model = get_vgg_model(input_shape)\n",
        "# model = get_alex_model(input_shape)\n",
        "model = get_inception_v3(\n",
        "    train_ds=train_ds,\n",
        "    train_steps=train_steps,\n",
        "    weights=weights,\n",
        "    freeze_layers_number=172,\n",
        "    input_shape=input_shape)\n",
        "# model = get_all_cnn_model(input_shape)\n",
        "# model.summary()\n",
        "\n",
        "# Train\n",
        "experiment_dir = os.path.join(args['experiments_dir'],\n",
        "                              datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "history = train(model=model,\n",
        "                train_ds=train_ds,\n",
        "                train_steps=train_steps,\n",
        "                val_ds=val_ds,\n",
        "                val_steps=val_count // args['batch_size'],\n",
        "                experiment_dir=experiment_dir,\n",
        "                epochs=500,\n",
        "                weights=weights)\n",
        "\n",
        "# Validate\n",
        "create_confusion_matrix(model, val_ds, val_count //\n",
        "                        args['batch_size'], CLASS_NAMES, experiment_dir)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arguments: {\"remote_project_dir\": \"drive/My Drive/diabetic-retinopathy-thesis\", \"project_dir\": \"/content\", \"image_dir\": \"/content/train_processed\", \"dataframe_path\": \"drive/My Drive/diabetic-retinopathy-thesis/trainLabels.csv\", \"experiments_dir\": \"drive/My Drive/diabetic-retinopathy-thesis/experiments\", \"img_size\": 299, \"batch_size\": 32}\n",
            "Found 35126 images\n",
            "Number of existed images is 35126\n",
            "Train dataframe size: 800 Validation dataframe size: 200\n",
            "Weights for each level:\n",
            " {0: 1.3675213675213675, 1: 14.814814814814815, 2: 6.349206349206349, 3: 47.05882352941177, 4: 44.44444444444444}\n",
            "Start training.\n",
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "Epoch 1/2\n",
            "1/1 [==============================] - 4s 4s/step - loss: 5.5476 - accuracy: 0.1875 - f1_score: 0.1900 - recall_multi: 0.8000 - top_2_accuracy: 0.4062 - val_loss: 1.6176 - val_accuracy: 0.0000e+00 - val_f1_score: 0.0000e+00 - val_recall_multi: 1.0000 - val_top_2_accuracy: 0.1562 - lr: 0.0010\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 4s 4s/step - loss: 12.3294 - accuracy: 0.0625 - f1_score: 0.0800 - recall_multi: 1.0000 - top_2_accuracy: 0.2188 - val_loss: 1.6060 - val_accuracy: 0.1875 - val_f1_score: 0.0857 - val_recall_multi: 0.7692 - val_top_2_accuracy: 0.5000 - lr: 0.0010\n",
            "Training finished.\n",
            "Creating confusion matrix.\n",
            "Confusion matrix was saved to drive/My Drive/diabetic-retinopathy-thesis/experiments/20200510-181603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7vnddzcvjMR",
        "colab_type": "text"
      },
      "source": [
        "# GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udY7sohtETlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboard.plugins.hparams import api as hp\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eeYLWP4uBkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_model(hparams):\n",
        "  train, train_count, val, val_count = create_datasets(train_df, val_df, hparams[HP_BATCH_NUM])\n",
        "  \n",
        "  base_model = InceptionV3(weights='imagenet', \n",
        "                           include_top=False, \n",
        "                           input_shape=get_input_shape())\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "\n",
        "  # let's add a fully-connected layer\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(4086, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  x = Dense(4086, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(hparams[HP_DROP])(x)\n",
        "  predictions = Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  # first: train only the top layers (which were randomly initialized)\n",
        "  # i.e. freeze all convolutional InceptionV3 layers\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "    \n",
        "  # compile the model (should be done *after* setting layers to non-trainable)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR]), loss='categorical_crossentropy')\n",
        "\n",
        "  # train the model on the new data for a few epochs\n",
        "  # model.fit(train, steps_per_epoch=train_count // hparams[HP_BATCH_NUM], epochs=3,\n",
        "  #         class_weight=weights)\n",
        "\n",
        "  # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "  for layer in model.layers[:hparams[HP_FREEZE_LAYERS]]:\n",
        "    layer.trainable = False\n",
        "  for layer in model.layers[hparams[HP_FREEZE_LAYERS]:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "  # we need to recompile the model for these modifications to take effect\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR]), \n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=get_metrics())\n",
        "  \n",
        "  model.fit(train, steps_per_epoch=train_count // hparams[HP_BATCH_NUM], \n",
        "            epochs=15) # Run with 1 epoch to speed things up for demo purposes\n",
        "  metrics = model.evaluate(val, steps=val_count // hparams[HP_BATCH_NUM])\n",
        "  return {k : metrics[i] for i, k in enumerate(model.metrics_names)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUGu6t40vq_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./logs/ \n",
        "\n",
        "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "HP_LR = hp.HParam('lr', hp.Discrete([1e-2]))\n",
        "# HP_LR = hp.HParam('lr', hp.Discrete([1e-2, 1e-3, 1e-4]))\n",
        "HP_BATCH_NUM = hp.HParam('batch_num', hp.Discrete([8]))\n",
        "# WITHOUT_MOMENTUM = 0.0\n",
        "# HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([WITHOUT_MOMENTUM, 0.5, 0.7, 0.9]))\n",
        "# HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([None]))\n",
        "HP_DROP = hp.HParam('drop', hp.Discrete([0.5]))\n",
        "HP_FREEZE_LAYERS = hp.HParam('freeze_layers', hp.Discrete([172]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "METRIC_F1 = 'f1_score'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_LR, HP_BATCH_NUM, HP_DROP, HP_FREEZE_LAYERS],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='accuracy'), \n",
        "             hp.Metric(METRIC_F1, display_name='f1_score')],\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv03N4-xwl7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "\n",
        "    # if hparams[HP_OPTIMIZER] == 'sgd':\n",
        "    #   hparams[HP_OPTIMIZER] = tf.keras.optimizers.SGD(\n",
        "    #     learning_rate=hparams[HP_LR],\n",
        "    #     momentum=hparams[HP_MOMENTUM]) \n",
        "    # else:\n",
        "    #   hparams[HP_OPTIMIZER] = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR])\n",
        "\n",
        "    metrics = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, metrics[METRIC_ACCURACY], step=1)\n",
        "    f1_scores_each_class = metrics[METRIC_F1]\n",
        "    f1_score_avg = sum(f1_scores_each_class) / len(f1_scores_each_class)\n",
        "    tf.summary.scalar(METRIC_F1,f1_score_avg, step=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgTprnhMw0ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for batch_num in HP_BATCH_NUM.domain.values:\n",
        "  for lr in HP_LR.domain.values:\n",
        "    for drop in HP_DROP.domain.values:\n",
        "      for fl in HP_FREEZE_LAYERS.domain.values:\n",
        "        hparams = {\n",
        "            HP_LR: lr,\n",
        "            HP_BATCH_NUM: batch_num,\n",
        "            HP_DROP: drop,\n",
        "            HP_FREEZE_LAYERS: fl\n",
        "        }\n",
        "        run_name = \"run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run('logs/hparam_tuning/' + run_name, hparams)\n",
        "        session_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxJ_AQGSTrBV",
        "colab_type": "text"
      },
      "source": [
        "# Other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWGuIDPbbt88",
        "colab_type": "text"
      },
      "source": [
        "Correctness of loss [CS231](http://cs231n.github.io/neural-networks-3/#sanitycheck) <font color='green'>OK</font>\n",
        "\n",
        "Loss should be equal -ln(1/len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoeLkz8oW_uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "-np.log(1/len(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gk2mMmthCR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df['level'].value_counts().plot(kind='bar', figsize=(5,2), title='Level distribution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUXrNAEJhH8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrrVpkAiS-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, label_batch = next(iter(val_ds))\n",
        "model.evaluate(image_batch, label_batch, BATCH_SIZE)\n",
        "predicted_probabilities = model.predict(image_batch)\n",
        "print('Predicted probabilities:', predicted_probabilities)\n",
        "predicted_labels = np.argmax(predicted_probabilities,axis=1)\n",
        "print('Labels:', np.argmax(label_batch,axis=1))\n",
        "print('Predicted labels:', predicted_labels)\n",
        "show_batch(image_batch.numpy(), label_batch.numpy(), number_to_show=20,predicted_labels=predicted_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp2jNhuFicJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir 'drive/My Drive/diabetic-retinopathy-thesis/Logs/20200507-001110'\n",
        "# %tensorboard --logdir 'logs'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jcQoPAuigOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_model = tf.keras.models.load_model(\n",
        "    os.path.join(PROJECT_DIR, 'models', 'model.17-0.76.hdf5'),\n",
        "    {'top_2_accuracy' : top_2_accuracy}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "diabetic-retinopathy-ml.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LhRDSErnGKIY",
        "Er8XzoNHGYXp",
        "ZOeej_fKIbnf"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}